\section{Matching}
% \subsection{Matching with Unbalanced Optimal Transport}

Following previous work~\cite{carion2020detr, zhu2020deformabledetr,ren2015fasterrcnn,redmon2016yolo,liu2016ssd}, we define a multi-task matching cost between a prediction $\hat{\mathbfit{y}}_i$ and a ground truth object $\mathbfit{y}_j$ as the composition of a classification loss ensuring that similar object classes are matched together and a localization loss ensuring the correspondence of the positions and shapes of the matched boxes $\label{eq:matching-loss}\mathcal{L}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) = \mathcal{L}_{\text{classification}}(\hat{\mathbfit{c}}_i, \mathbfit{c}_j) + \mathcal{L}_{\text{localization}}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j)$. %
% a weighted sum of a cross-entropy ($\mathrm{CE}$) classification loss and two localization losses, the Generalized Intersection over Union ($\mathrm{GIoU}$) and the $\ell^1$ loss:
% \begin{equation}
% \label{eq:matching-loss}
%     \mathcal{L}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) = \mathcal{L}_{\text{classification}}(\hat{\mathbfit{c}}_i, \mathbfit{c}_j) + \mathcal{L}_{\text{localization}}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j). 
% % \begin{array}{rcl}
% %     \mathcal{L}_{\text{match}}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) &=& \lambda_{\mathrm{CE}} \mathcal{L}_{\mathrm{CE}}(\hat{\mathbfit{c}}_i, \mathbfit{c}_j) \\
% %     &&+\; \lambda_{\ell^1} \mathcal{L}_{\ell^1}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j) \\
% %     &&+\; \lambda_{\mathrm{GIoU}} \mathcal{L}_{\mathrm{GIoU}}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j).
% % \end{array}
% \end{equation}
Most models, however, do not use the same loss to determine the matches as the one used to train the model. We therefore refer to these two losses as $\mathcal{L}_{\text{match}}$ and $\mathcal{L}_{\text{train}}$. The training procedure is the following: first find a match $\hat{\mathbfit{P}}$ given a matching strategy and matching cost $\mathcal{L}_{\text{match}}$, then compute the loss $N_p \sum_{i=1}^{N_p} \sum_{j=1}^{N_g} \hat{P}_{ij} \mathcal{L}_{\text{train}}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j)$ where the particular training loss for the background ground truth includes only a classification term $\mathcal{L}_{\text{train}}(\hat{\mathbfit{y}}_i, \varnothing) = \mathcal{L}_{\text{classification}}(\hat{\mathbfit{c}}_i, \varnothing)$.

\subsection{Detection Transformer (DETR)} The object detection is performed by matching the predictions to the ground truth boxes with the \emph{Hungarian algorithm} applied to the loss $\mathcal{L}_{\text{match}}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) = \lambda_{\text{prob}}(1 - \left< \hat{\mathbfit{c}}_i, \mathbfit{c}_j \right>) + \lambda_{\ell^1} \lVert\hat{\mathbfit{b}}_i- \mathbfit{b}_j\rVert_1 +\lambda_{\mathrm{GIoU}}(1-\mathrm{GIoU}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j))$ (Definition~\ref{def:lap}). To do so, the number of predictions and ground truth boxes must be of the same size. This is achieved by padding the ground truths with $(N_p - N_g)$ dummy \emph{background} $\varnothing$ objects. Essentially, this is the same as what is developed in Proposition~\ref{prop:lap}. The obtained match is then used to define an object-specific loss, where each matched prediction is pushed toward its corresponding ground truth object. The predictions that are not matched to a ground truth object are considered to be matched with the background and are pushed to predict the background class. The training loss uses the cross-entropy ($\mathrm{CE}$) for classification: $\mathcal{L}_{\text{train}}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) = \lambda_{\mathrm{CE}} \mathcal{L}_{\mathrm{CE}}(\hat{\mathbfit{c}}_i, \mathbfit{c}_j) + \lambda_{\ell^1} \lVert\hat{\mathbfit{b}}_i- \mathbfit{b}_j\rVert_1 +\lambda_{\mathrm{GIoU}}(1-\mathrm{GIoU}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j))$. By directly applying Proposition~\ref{prop:lap} and adding entropic regularization (Definition~\ref{def:rOT}), we can use \emph{Sinkhorn's algorithm} and push each prediction $\hat{\mathbfit{y}}_i$ to ground truth $\mathbfit{y}_j$ according to weight $\hat{P}_{i,j}$. In particular, for any non-zero $\hat{P}_{i,N_g+1} \neq 0$, the prediction $\hat{\mathbfit{y}}_i$ is pushed toward the background $\mathbfit{y}_{N_g+1} = \varnothing$ with weight $\hat{P}_{i,N_g+1}$.

\subsection{Single Shot MultiBox Detector (SSD)} The Single Shot MultiBox Detector~\cite{liu2016ssd} uses a matching cost only comprised of the $\mathrm{IoU}$ between the fixed anchor boxes $\tilde{\mathbfit{b}}_i$ and the ground truth boxes: $\mathcal{L}_{\text{match}}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) = 1-\mathrm{IoU}(\tilde{\mathbfit{b}}_i, \mathbfit{b}_j)$ (the $\mathrm{GIoU}$ was not published yet~\cite{giou}). Each ground truth is first matched toward the closest anchor box. Anchor boxes are then matched to a ground truth object if the matching cost is below a threshold of 0.5. In our framework, this corresponds to applying $\tau_1=0$ and $\tau_2\to \infty$ for the first phase and then $\tau_1\to \infty$ and $\tau_2=0$ with $c_{\varnothing} = 0.5$ (see Proposition~\ref{prop:threshold}). Here again, by adding entropic regularization (Definition~\ref{def:ruOT}), we can solve this using a \emph{scaling algorithm}. We furthermore can play with the parameters $\tau_1$ and $\tau_2$ to make the matching tend slightly more towards a matching done with the \emph{Hungarian algorithm} (Figure~\ref{fig:params-influence}). Again, the training uses a different loss than the matching, in particular $\mathcal{L}_{\text{train}}(\hat{\mathbfit{y}}_i, \mathbfit{y}_j) = \lambda_{\mathrm{CE}} \mathcal{L}_{\mathrm{CE}}(\hat{\mathbfit{c}}_i, \mathbfit{c}_j) + \lambda_{\text{smooth }\ell^1} \mathcal{L}_{\text{smooth }\ell^1}(\hat{\mathbfit{b}}_i, \mathbfit{b}_j)$.

% \subsubsection{Matching with anchor boxes} 
% SSD uses fixed anchor boxes instead of predicted boxes in the matching computation.
% If SSD directly matched predictions with the closest ground truth in the second matching phase, all predictions might fall under the matching threshold or match the same ground truth.
% Moreover, Figure \ref{fig:general-min2} illustrates that it is necessary to have a sufficient number of anchor boxes to ensure that each ground truth is matched towards a different anchor box in the first matching phase. 

\subsubsection{Hard Negative Mining} Instead of using all negative examples $N_{\text{neg}} = (N_p - N_g)$ (predictions matched to background), the method sorts them using the highest confidence loss $\mathcal{L}_{\mathrm{CE}}(\hat{\mathbfit{c}}_i,\varnothing)$ and picks the top ones so that the ratio between the hard negatives and positives $N_{\text{pos}} = N_g$ is at most 3 to 1.
Since $\hat{\mathbfit{P}}$ is non-binary, we define the number of negatives and positives to be the sum of the matches to the background $N_{\text{neg}} = N_p\sum_{i=1}^{N_p} \hat{P}_{i,(N_g+1)}$ and to the ground truth objects $N_{\text{pos}} = N_p\sum_{j=1}^{N_g}\sum_{i=1}^{N_p} \hat{P}_{ij}$. We verify that for any $\mathbfit{P} \in \mathcal{U}(\mathbfit{\alpha},\mathbfit{\beta})$, we have the same number of positives and negatives as the initial model: $N_{\text{neg}} = (N_p-N_g)$ and
$N_{\text{pos}} = N_g$. Hence, hard negatives are the $K$ predictions with the highest confidence loss $\hat{P}_{k,(N_g+1)} \mathcal{L}_{\mathrm{CE}}(\hat{\mathbfit{c}}_k, \varnothing)$ such that the mass of kept negatives is at most triple the number of positives: $N_p \sum_{k=1}^K \hat{P}_{k,(N_g+1)}^{s} \leq 3 N_{\text{pos}}$, where $\hat{\mathbfit{P}}^s$ is a permutation of transport matrix $\hat{\mathbfit{P}}$ with rows sorted by highest confidence loss.





