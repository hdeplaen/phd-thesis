@ARTICLE{USPS,
author={Hull, Jonathan J.},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={A database for handwritten text recognition research},
year={1994},
volume={16},
number={5},
pages={550-554},
doi={10.1109/34.291440},
ISSN={1939-3539},
month={5},}

@book{suykens:worldsci2002,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Singapore},
  author = {Suykens, Johan A. K. and Gestel, Tony Van and Brabanter, Jos De and Moor, Bart De and Vandewalle, Joos},
  biburl = {https://www.bibsonomy.org/bibtex/2fe7870a85e6675a5fb937feea9c137a5/idsia},
  interhash = {ca1ec63dc587492eb2746977e2669187},
  intrahash = {fe7870a85e6675a5fb937feea9c137a5},
  keywords = {imported},
  publisher = {World Scientific},
  timestamp = {2008-03-11T14:52:37.000+0100},
  title = {Least Squares Support Vector Machines},
  year = 2002
}



@book{rasmussen:williams:2006,
  added-at = {2009-03-05T08:49:50.000+0100},
  author = {Rasmussen, Carl E. and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/26771eaebbee7d852934f29aa33dea971/bcao},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {6771eaebbee7d852934f29aa33dea971},
  keywords = {},
  publisher = {MIT Press},
  timestamp = {2009-03-05T08:49:50.000+0100},
  title = {Gaussian Processes for Machine Learning},
  year = 2006
}



@incollection{Mairal2016,
title = {End-to-End Kernel Learning with Supervised Convolutional Kernel Networks},
author = {Mairal, Julien},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {1399--1407},
year = {2016},
publisher = {Curran Associates, Inc.},
//url ={http://papers.nips.cc/paper/6184-end-to-end-kernel-learning-with-supervised-convolutional-kernel-networks.pdf}
}

@misc{GenRKM,
title = {Generative Restricted Kernel Machines},
archivePrefix = {arXiv},
eprint = {arXiv:1906.08144},
primaryClass = {math.LG},
author = {Pandey, Arun and  Schreurs, Joachim and Suykens, Johan A. K.},
}

@misc{KNN_OT,
title = {Monge’s Optimal Transport Distance for Image Classification},
archivePrefix = {arXiv},
eprint = {arXiv:1612.00181},
primaryClass = {cs.CV},
author = {Snow, Michael and  Van Lent, Jan},
}


@incollection{Lightspeed,
title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
author = {Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 26},
pages = {2292--2300},
year = {2013},
publisher = {Curran Associates, Inc.},
//url ={http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf}
}


@article{LSSVM,
author = {Suykens, Johan A. K. and Vandewalle, Joos},
title = {Least Squares Support Vector Machine Classifiers},
year = {1999},
issue_date = {June 1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {1370-4621},
//url = {https://doi.org/10.1023/A:1018628609742},
doi = {10.1023/A:1018628609742},
journal = {Neural Process. Lett.},
month = jun,
pages = {293–300},
numpages = {8},
keywords = {linear least squares, radial basis function kernel, support vector machines, classification}
}

 

@incollection{SlicedWasserstein,
title = {Generalized Sliced Wasserstein Distances},
author = {Kolouri, Soheil and Nadjahi, Kimia and Simsekli, Umut and Badeau, Roland and Rohde, Gustavo},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {261--272},
year = {2019},
publisher = {Curran Associates, Inc.},
//url ={http://papers.nips.cc/paper/8319-generalized-sliced-wasserstein-distances.pdf}
}

@article{Mairal,
    author = {Chen, Dexiong and Jacob, Laurent and Mairal, Julien},
    title = "{Biological sequence modeling with convolutional kernel networks}",
    journal = {Bioinformatics},
    volume = {35},
    number = {18},
    pages = {3294-3302},
    year = {2019},
    month = {02},
     doi = {10.1093/bioinformatics/btz094},
    //url ={https://doi.org/10.1093/bioinformatics/btz094},
}

@article{Reams1999,
abstract = {Let A = (aij) be an n × n symmetric matrix with all positive entries and just one positive eigenvalue. Bapat proved then that the Hadamard inverse of A, given by Ao(-1)= (1/aijis positive semidefinite. We show that if moreover A is invertible then Ao(-1)is positive definite. We use this result to obtain a simple proof that with the same hypotheses on A, except that all the diagonal entries of A are zero, the Hadamard square root of A, given by Ao1/2= (a1/2ij), has just one positive eigenvalue and is invertible. Finally, we show that if A is any positive semidefinite matrix and B is almost positive definite and invertible then A o B ≻ +(combining low line) (1/eTB-1e)A. {\textcopyright} 1999 Elsevier Science Inc. All rights reserved.},
author = {Reams, Robert},
doi = {10.1016/S0024-3795(98)10162-3},
file = {:Users/hdeplaen/Desktop/1-s2.0-S0024379598101623-main.pdf:pdf},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
keywords = {Almost positive semidefinite,Distance matrix,Hadamard inverse,Hadamard product,Hadamard square root,Positive semidefinite},
number = {1-3},
pages = {35--43},
title = {{Hadamard inverses, square roots and products of almost semidefinite matrices}},
volume = {288},
year = {1999}
}
@article{Johnson1987,
abstract = {It is known that if A is positive definite Hermitian, then A{\textperiodcentered}A -1 ≥I in the positive semidefinite ordering. Our principal new result is a converse to this inequality: under certain weak regularity assumptions about a function F on the positive definite matrices, A{\textperiodcentered}F(A)≥AF(A) for all positive definite A if and only if F(A) is a positive multiple of A -1 . In addition to the inequality A{\textperiodcentered}A -1 ≥I, it is known that A{\textperiodcentered}A -1T ≥I and, stronger, that $\lambda$ min (A{\textperiodcentered}B)≥$\lambda$ min (AB T ), for A, B positive definite Hermitian. We also show that $\lambda$ min (A{\textperiodcentered}B)≥$\lambda$ min (AB) and note that $\lambda$ min (AB) and $\lambda$ min (AB T ) can be quite different for A, B positive definite Hermitian. We utilize a simple technique for dealing with the Hadamard product, which relates it to the conventional product and which allows us to give especially simple proofs of the closure of the positive definites under Hadamard multiplication and of the inequalities mentioned. {\textcopyright} 1987.},
author = {Johnson, Charles R. and Elsner, Ludwig},
doi = {10.1016/0024-3795(87)90260-6},
file = {:Users/hdeplaen/Desktop/82215759.pdf:pdf},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
number = {C},
pages = {231--240},
title = {{The relationship between Hadamard and conventional multiplication for positive definite matrices}},
volume = {92},
year = {1987}
}
@article{Kurata2016,
abstract = {By a hollow symmetric matrix we mean a symmetric matrix with zero diagonal elements. The notion contains those of predistance matrix and Euclidean distance matrix as its special cases. By a centered symmetric matrix we mean a symmetric matrix with zero row (and hence column) sums. There is a one-toone correspondence between the classes of hollow symmetric matrices and centered symmetric matrices, and thus with any hollow symmetric matrix D we may associate a centered symmetric matrix B, and vice versa. This correspondence extends a similar correspondence between Euclidean distance matrices and positive semidefinite matrices with zero row and column sums.We show that if B has rank r, then the corresponding D must have rank r, r + 1 or r + 2. We give a complete characterization of the three cases.We obtain formulas for the Moore-Penrose inverse D},
author = {Kurata, Hiroshi and Bapat, Ravindra B.},
doi = {10.1515/spma-2016-0028},
file = {:Users/hdeplaen/Desktop/[Special Matrices] Moore-Penrose inverse of a hollow symmetricmatrix and a predistance matrix.pdf:pdf},
issn = {23007451},
journal = {Special Matrices},
keywords = {Euclidean distance matrix,Hollow matrix,Laplacian matrix,Moore-Penrose inverse,Positive semidefinite matrix,Predistance matrix,Tree},
number = {1},
pages = {270--282},
title = {{Moore-Penrose inverse of a hollow symmetric matrix and a predistance matrix}},
volume = {4},
year = {2016}
}
@article{Nicoara2000,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0610529v1},
author = {Nicoara, Teodor Banicaremus},
eprint = {0610529v1},
file = {:Users/hdeplaen/Desktop/0610529.pdf:pdf},
number = {4},
pages = {1--22},
primaryClass = {arXiv:math},
title = {{Quantum groups and hadamard matrices}},
volume = {65},
year = {2000}
}
@article{Sivakumar2015,
author = {Sivakumar, K. C. and Meenakshi, Ar. and Choudhury, Projesh Nath},
doi = {10.13001/1081-3810.2969},
file = {:Users/hdeplaen/Desktop/Almost Definite Matrices.pdf:pdf},
journal = {Electronic Journal of Linear Algebra},
keywords = {almost definite matrix,group,moore-penrose inverse,range-symmetric matrix},
number = {1},
pages = {102--119},
title = {{Almost Definite Matrices Revisited}},
volume = {29},
year = {2015}
}
@article{Creswell2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.07035v1},
author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A and Ieee, Member},
eprint = {arXiv:1710.07035v1},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Creswell et al. - 2017 - Generative Adversarial Networks An Overview.pdf:pdf},
number = {April},
pages = {1--14},
title = {{Generative Adversarial Networks : An Overview}},
year = {2017}
}
@article{Suykens2003,
abstract = {In this paper, we present a simple and straightforward primal-dual support vector machine formulation to the problem of principal component analysis (PCA) in dual variables. By considering a mapping to a high-dimensional feature space and application of the kernel trick (Mercer theorem), kernel PCA is obtained as introduced by Scholkopf et al. (2002). While least squares support vector machine classifiers have a natural link with the kernel Fisher discriminant analysis (minimizing the within class scatter around targets +1 and -1), for PCA analysis one can take the interpretation of a one-class modeling problem with zero target value around which one maximizes the variance. The score variables are interpreted as error variables within the problem formulation. In this way primal-dual constrained optimization problem interpretations to the linear and kernel PCA analysis are obtained in a similar style as for least square-support vector machine classifiers.},
author = {Suykens, Johan A. K. and {Van Gestel}, T. and Vandewalle, J. and {De Moor}, B.},
doi = {10.1109/TNN.2003.809414},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Suykens et al. - 2003 - A support vector machine formulation to PCA analysis and its kernel version.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Kernel methods,Kernel principal component analysis (PCA),Least squares-support vector machine (LS-SVM),PCA analysis,SVMs},
number = {2},
pages = {447--450},
publisher = {IEEE},
title = {{A support vector machine formulation to PCA analysis and its kernel version}},
volume = {14},
year = {2003}
}
@article{Flamary2016,
abstract = {Wasserstein Discriminant Analysis (WDA) is a new supervised method that can improve classification of high-dimensional data by computing a suitable linear map onto a lower dimensional subspace. Following the blueprint of classical Linear Discriminant Analysis (LDA), WDA selects the projection matrix that maximizes the ratio of two quantities: the dispersion of projected points coming from different classes, divided by the dispersion of projected points coming from the same class. To quantify dispersion, WDA uses regularized Wasserstein distances, rather than cross-variance measures which have been usually considered, notably in LDA. Thanks to the the underlying principles of optimal transport, WDA is able to capture both global (at distribution scale) and local (at samples scale) interactions between classes. Regularized Wasserstein distances can be computed using the Sinkhorn matrix scaling algorithm; We show that the optimization of WDA can be tackled using automatic differentiation of Sinkhorn iterations. Numerical experiments show promising results both in terms of prediction and visualization on toy examples and real life datasets such as MNIST and on deep features obtained from a subset of the Caltech dataset.},
archivePrefix = {arXiv},
arxivId = {1608.08063},
author = {Flamary, R{\'{e}}mi and Cuturi, Marco and Courty, Nicolas and Rakotomamonjy, Alain},
doi = {10.1007/s10994-018-5717-1},
eprint = {1608.08063},
file = {:Users/hdeplaen/Desktop/1608.08063v1.pdf:pdf},
month = {aug},
number = {12},
title = {{Wasserstein Discriminant Analysis}},
//url ={http://arxiv.org/abs/1608.08063 http://dx.doi.org/10.1007/s10994-018-5717-1},
volume = {107},
year = {2016}
}
@article{Fischer2015,
abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. They have attracted much attention as building blocks for the multi-layer learning systems called deep belief networks, and variants and ex-tensions of RBMs have found application in a wide range of pattern recognition tasks. This tutorial introduces RBMs from the viewpoint of Markov random fields, starting with the re-quired concepts of undirected graphical models. Different learning algorithms for RBMs, in-cluding contrastive divergence learning and parallel tempering, are discussed. As sampling from RBMs, and therefore also most of their learning algorithms, are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and MCMC techniques is provided. Experiments demonstrate relevant aspects of RBM training.},
author = {Fischer, Asja},
doi = {10.1007/s13218-015-0371-2},
file = {:Users/hdeplaen/Desktop/TRBMAI.pdf:pdf},
issn = {0933-1875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
keywords = {contrastive divergence learning,gibbs sampling,markov chains,markov random fields,neural networks,parallel tempering,restricted boltzmann machines},
number = {4},
pages = {441--444},
title = {{Training Restricted Boltzmann Machines}},
volume = {29},
year = {2015}
}
@article{Lee2016,
author = {Lee, Jason D and Jordan, Michael I},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Jordan - 2016 - Gradient Descent Only Converges to Minimizers.pdf:pdf},
keywords = {gradient descent,local minimum,non-convex,saddle points},
number = {Equation 1},
pages = {1--12},
title = {{Gradient Descent Only Converges to Minimizers}},
volume = {49},
year = {2016}
}
@article{Alaiz2018,
abstract = {In this paper, Kernel PCA is reinterpreted as the solution to a convex optimization problem. Actually, there is a constrained convex problem for each principal component, so that the constraints guarantee that the principal component is indeed a solution, and not a mere saddle point. Although these insights do not imply any algorithmic improvement, they can be used to further understand the method, formulate possible extensions and properly address them. As an example, a new convex optimization problem for semi-supervised classification is proposed, which seems particularly well-suited whenever the number of known labels is small. Our formulation resembles a Least Squares SVM problem with a regularization parameter multiplied by a negative sign, combined with a variational principle for Kernel PCA. Our primal optimization principle for semi-supervised learning is solved in terms of the Lagrange multipliers. Numerical experiments in several classification tasks illustrate the performance of the proposed model in problems with only a few labeled data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.06811v1},
author = {Ala{\'{i}}z, Carlos M. and Fanuel, Michael and Suykens, Johan A. K.},
doi = {10.1109/TNNLS.2017.2709838},
eprint = {arXiv:1610.06811v1},
file = {:Users/hdeplaen/Desktop/Project Micha{\"{e}}l/1610.06811.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Kernel principal component analysis (KPCA),kernel spectral clustering,semisupervised learning,support vector machines},
number = {8},
pages = {3863--3869},
title = {{Convex formulation for kernel PCA and its use in semisupervised learning}},
volume = {29},
year = {2018}
}
@article{Angiulli2005,
abstract = {We present a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule. The algorithm, called FCNN rule, has some desirable properties. Indeed, it is order independent and has sub-quadratic worst case time complexity, while it requires few iterations to converge, and it is likely to select points very close to the de-cision boundary. We compare the FCNN rule with state of the art competence preservation algorithms on large multidimensional train-ing sets, showing that it outperforms existing methods in terms of learning speed and learn-ing scaling behavior, and in terms of size of the model, while it guarantees a comparable prediction accuracy.},
author = {Angiulli, Fabrizio},
doi = {10.1145/1102351.1102355},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Angiulli - 2005 - Fast condensed nearest neighbor rule.pdf:pdf},
isbn = {1595931805},
issn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {25--32},
title = {{Fast condensed nearest neighbor rule}},
//url ={http://portal.acm.org/citation.cfm?doid=1102351.1102355},
year = {2005}
}
@inproceedings{Ong:2004:LNK:1015330.1015443,
address = {New York, NY, USA},
author = {Ong, Cheng Soon and Mary, Xavier and Canu, St{\'{e}}phane and Smola, Alexander J},
booktitle = {Proceedings of the Twenty-first International Conference on Machine Learning},
doi = {10.1145/1015330.1015443},
file = {:Users/hdeplaen/Desktop/392.pdf:pdf},
isbn = {1-58113-838-5},
pages = {81----},
publisher = {ACM},
series = {ICML '04},
title = {{Learning with Non-positive Kernels}},
//url ={http://doi.acm.org/10.1145/1015330.1015443},
year = {2004}
}
@article{Courty2017,
abstract = {The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.},
archivePrefix = {arXiv},
arxivId = {1710.07457},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Ducoffe, M{\'{e}}lanie},
eprint = {1710.07457},
file = {:Users/hdeplaen/Desktop/1710.07457.pdf:pdf},
month = {oct},
pages = {1--10},
title = {{Learning Wasserstein Embeddings}},
//url ={http://arxiv.org/abs/1710.07457},
volume = {1},
year = {2017}
}
@article{Weiss1996,
author = {Weiss, Joan O and Mackta, Jayne S},
doi = {10.1007/978-3-642-35289-8_32},
file = {:Users/hdeplaen/Desktop/guideTR.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {364235288X},
pmid = {1000104336},
title = {{Starting {\&} sustaining genetic support groups}},
year = {1996}
}
@article{Luise2018,
abstract = {Applications of optimal transport have recently gained remarkable attention thanks to the computational advantages of entropic regularization. However, in most situations the Sinkhorn approximation of the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn distance, proving that it enjoys the same smoothness as its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula allows us to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.},
archivePrefix = {arXiv},
arxivId = {1805.11897},
author = {Luise, Giulia and Rudi, Alessandro and Pontil, Massimiliano and Ciliberto, Carlo},
eprint = {1805.11897},
file = {:Users/hdeplaen/Desktop/7827-differential-properties-of-sinkhorn-approximation-for-learning-with-wasserstein-distance.pdf:pdf},
number = {NeurIPS},
pages = {1--12},
title = {{Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance}},
//url ={http://arxiv.org/abs/1805.11897},
year = {2018}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:pdf},
month = {jun},
pages = {1--9},
title = {{Generative Adversarial Networks}},
//url ={http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Salimans2016,
author = {Salimans, Tim and Goodfellow, Ian and Cheung, Vicki and Radford, Alec and Chen, Xi},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:pdf},
number = {Nips},
pages = {1--9},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{Schreurs2018,
abstract = {Kernel PCA has shown to be a powerful feature extractor within many applications. Using the Restricted Kernel Machine formu- lation, a representation using visible and hidden units is obtained. This enables the exploration of new insights and connections between Restricted Boltzmann machines and kernel methods. This paper explores these con- nections, introducing a generative kernel PCA which can be used to gen- erate new data, as well as denoise a given training dataset. This in a non-probabilistic setting. Moreover, relations with linear PCA and a pre- image reconstruction method are introduced in this paper. 1},
author = {Schreurs, Joachim and Suykens, Johan A. K.},
file = {:Users/hdeplaen/Desktop/es2018-173.pdf:pdf},
isbn = {9782875870476},
number = {April},
pages = {2012--2017},
title = {{Generative Kernel PCA}},
volume = {002},
year = {2018}
}
@article{Collobert2006,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L{\'{e}}on},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Collobert et al. - 2006 - Large Scale Transductive SVMs{\_}JMLR2006.pdf.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cccp,semi-supervised learning,transduction,transductive svms},
pages = {1687--1712},
title = {{Large Scale Transductive SVMs{\_}JMLR2006.pdf}},
volume = {7},
year = {2006}
}
@inproceedings{Li2007,
address = {New York, New York, USA},
author = {Li, Fuxin and Yang, Jian and Wang, Jue},
booktitle = {Proceedings of the 24th international conference on Machine learning - ICML '07},
doi = {10.1145/1273496.1273561},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Li, Yang, Wang - 2007 - A transductive framework of distance metric learning by spectral dimensionality reduction.pdf:pdf},
isbn = {9781595937933},
pages = {513--520},
publisher = {ACM Press},
title = {{A transductive framework of distance metric learning by spectral dimensionality reduction}},
//url ={http://portal.acm.org/citation.cfm?doid=1273496.1273561},
year = {2007}
}
@article{Alzate2010,
abstract = {A new formulation for multiway spectral clustering is proposed. This method corresponds to a weighted kernel principal component analysis (PCA) approach based on primal-dual least-squares support vector machine (LS-SVM) formulations. The formulation allows the extension to out-of-sample points. In this way, the proposed clustering model can be trained, validated, and tested. The clustering information is contained on the eigendecomposition of a modified similarity matrix derived from the data. This eigenvalue problem corresponds to the dual solution of a primal optimization problem formulated in a high-dimensional feature space. A model selection criterion called the Balanced Line Fit (BLF) is also proposed. This criterion is based on the out-of-sample extension and exploits the structure of the eigenvectors and the corresponding projections when the clusters are well formed. The BLF criterion can be used to obtain clustering parameters in a learning framework. Experimental results with difficult toy problems and image segmentation show improved performance in terms of generalization to new samples and computation times.},
author = {Alzate, Carlos and Suykens, Johan A. K.},
doi = {10.1109/TPAMI.2008.292},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Alzate, Suykens - 2010 - Multiway spectral clustering with out-of-sample extensions through weighted kernel PCA.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Kernel principal component analysis,Model selection.,Out-of-sample extensions,Spectral clustering},
number = {2},
pages = {335--347},
title = {{Multiway spectral clustering with out-of-sample extensions through weighted kernel PCA}},
volume = {32},
year = {2010}
}
@article{Journal2017,
author = {Journal, Econometrics},
doi = {10.1111/ectj.12083},
file = {:Users/hdeplaen/Desktop/ectj00c1.pdf:pdf},
keywords = {convex analysis,discrete choice,matching,optimal transport,quantile},
pages = {1--11},
title = {{A survey of some recent applications of optimal transport methods to econometrics}},
volume = {20},
year = {2017}
}
@article{Karras2017,
abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\^{}}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
archivePrefix = {arXiv},
arxivId = {1710.10196},
author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
eprint = {1710.10196},
file = {:Users/hdeplaen/Desktop/1710.10196.pdf:pdf},
month = {oct},
pages = {1--26},
title = {{Progressive Growing of GANs for Improved Quality, Stability, and Variation}},
//url ={http://arxiv.org/abs/1710.10196},
year = {2017}
}
@article{Liu2008,
abstract = {In this paper, we study semisupervised linear dimensionality reduction. Beyond conventional supervised methods which merely consider labeled instances, the semisupervised scheme allows to leverage abundant and ample unlabeled instances into learning so as to achieve better generalization performance. Under semisupervised settings, our objective is to learn a smooth as well as discriminative subspace and linear dimensionality reduction is thus achieved by mapping all samples into the subspace. Specifically, we present the transductive component analysis (TCA) algorithm to generate such a subspace founded on a graph-theoretic framework. Considering TCA is nonorthogonal, we further present the orthogonal transductive component analysis (OTCA) algorithm to iteratively produce a series of orthogonal basis vectors. OTCA has better discriminating power than TCA. Experiments carried out on synthetic and real-world datasets by OTCA show a clear improvement over the results of representative dimensionality reduction algorithms.},
author = {Liu, Wei and Tao, Dacheng and Liu, Jianzhuang},
doi = {10.1109/ICDM.2008.101},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Tao, Liu - 2008 - Transductive component analysis.pdf:pdf},
isbn = {9780769535029},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
pages = {433--442},
publisher = {IEEE},
title = {{Transductive component analysis}},
year = {2008}
}
@article{Kaieda2004,
abstract = {In a fuzzy classifier with ellipsoidal regions, a fuzzy rule, which is based on the Mahalanobis distance, is defined for each class. Then the fuzzy rules are tuned so that the recognition rate of the training data is maximized. In most cases, one fuzzy rule per one class is enough to obtain high generalization ability. But in some cases, we need to partition the class data to define more than one rule per class. In this paper, instead of partitioning the class data, we map the input space into the high dimensional feature space and then generate a fuzzy classifier with ellipsoidal regions in the feature space. We call this classifier kernel fuzzy classifier with ellipsoidal regions. To speed up training, first we select independent training data that span the subspace in the feature space and calculate the kernel principal components. By this method, we can avoid using singular value decomposition, which leads to training speedup. In the feature space, training data are usually degenerate. Namely, the space spanned by the mapped training data is a proper subspace. Thus, if the mapped test data are in the complementary subspace, the Mahalanobis distance may become erroneous and thus the probability of misclassification becomes high. To overcome this problem, we propose transductive training: in training, we add basis vectors of the input space as unlabelled data; and in classification, if mapped unknown data are not in the subspace we expand the subspace so that they are included. We demonstrate the effectiveness of our method by computer simulations. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Kaieda, Kenichi and Abe, Shigeo},
doi = {10.1016/j.ijar.2004.03.001},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Kaieda, Abe - 2004 - KPCA-based training of a kernel fuzzy classifier with ellipsoidal regions.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
month = {nov},
number = {3},
pages = {189--217},
title = {{KPCA-based training of a kernel fuzzy classifier with ellipsoidal regions}},
//url ={https://linkinghub.elsevier.com/retrieve/pii/S0888613X04000337},
volume = {37},
year = {2004}
}
@inproceedings{Guyon2011,
abstract = {We organized a data mining challenge in “unsupervised and transfer learning” (the UTL challenge), in collaboration with the DARPA Deep Learning program. The goal of this year's challenge was to learn good data representations that can be re-used across tasks by building models that capture regularities of the input space. The representations provided by the participants were evaluated by the organizers on supervised learning “target tasks”, which were unknown to the participants. In a first phase of the challenge, the competitors were given only unlabeled data to learn their data representation. In a second phase of the challenge, the competitors were also provided with a limited amount of labeled data from “source tasks”, distinct from the “target tasks”. We made available large datasets from various application domains: handwriting recognition, image recognition, video processing, text processing, and ecology. The results indicate that learned data representation yield results significantly better than what can be achieved with raw data or data preprocessed with standard normalizations and functional transforms. The UTL challenge is part of the IJCNN 2011 competition program1. The website of the challenge remains open for submission of new methods beyond the termination of the challenge as a resource for students and researchers2.},
author = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Aha, David W.},
booktitle = {The 2011 International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2011.6033302},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Guyon et al. - 2011 - Unsupervised and transfer learning challenge.pdf:pdf},
isbn = {978-1-4244-9635-8},
month = {jul},
pages = {793--800},
publisher = {IEEE},
title = {{Unsupervised and transfer learning challenge}},
//url ={http://ieeexplore.ieee.org/document/6033302/},
year = {2011}
}
@article{Courty2015,
abstract = {Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.},
archivePrefix = {arXiv},
arxivId = {1507.00504},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis and Rakotomamonjy, Alain},
eprint = {1507.00504},
file = {:Users/hdeplaen/Desktop/1507.00504.pdf:pdf},
month = {jul},
number = {X},
pages = {1--14},
title = {{Optimal Transport for Domain Adaptation}},
//url ={http://arxiv.org/abs/1507.00504},
volume = {X},
year = {2015}
}
@article{Gulrajani,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.00028v3},
author = {Gulrajani, Ishaan},
eprint = {arXiv:1704.00028v3},
file = {:Users/hdeplaen/Desktop/1704.00028.pdf:pdf},
title = {{Improved Training of Wasserstein GANs}}
}
@article{Zhao2016,
abstract = {We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.},
archivePrefix = {arXiv},
arxivId = {1609.03126},
author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
eprint = {1609.03126},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Mathieu, LeCun - 2016 - Energy-based Generative Adversarial Network.pdf:pdf},
month = {sep},
number = {2014},
pages = {1--17},
title = {{Energy-based Generative Adversarial Network}},
//url ={http://arxiv.org/abs/1609.03126},
year = {2016}
}
@inproceedings{pmlr-v32-cuturi14,
abstract = {We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter $\backslash$citepagueh2011barycenters,rabin2012, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of $\backslash$citetcuturi2013sinkhorn, we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.},
address = {Bejing, China},
author = {Cuturi, Marco and Doucet, Arnaud},
booktitle = {Proceedings of the 31st International Conference on Machine Learning},
editor = {Xing, Eric P and Jebara, Tony},
file = {:Users/hdeplaen/Desktop/cuturi14.pdf:pdf},
number = {2},
pages = {685--693},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Fast Computation of Wasserstein Barycenters}},
//url ={http://proceedings.mlr.press/v32/cuturi14.html},
volume = {32},
year = {2014}
}
@article{Gestel2002,
author = {Gestel, T Van and Suykens, Johan A. K. and Lanckriet, G. and Lambrechts, A. and Moor, B. De and Vandewalle, J.},
doi = {10.1162/089976602753633411},
file = {:Users/hdeplaen/Desktop/ncomp{\_}02.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {may},
number = {5},
pages = {1115--1147},
title = {{Bayesian Framework for Least-Squares Support Vector Machine Classifiers, Gaussian Processes, and Kernel Fisher Discriminant Analysis}},
//url ={http://www.mitpressjournals.org/doi/10.1162/089976602753633411},
volume = {14},
year = {2002}
}
@article{Cohen2016,
abstract = {It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.},
archivePrefix = {arXiv},
arxivId = {1612.08498},
author = {Cohen, Taco S. and Welling, Max},
eprint = {1612.08498},
file = {:Users/hdeplaen/Desktop/1612.08498.pdf:pdf},
number = {1990},
pages = {1--14},
title = {{Steerable CNNs}},
//url ={http://arxiv.org/abs/1612.08498},
year = {2016}
}
@article{Theis2015,
abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
archivePrefix = {arXiv},
arxivId = {1511.01844},
author = {Theis, Lucas and van den Oord, A{\"{a}}ron and Bethge, Matthias},
doi = {10.1021/jp0466845},
eprint = {1511.01844},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Theis, Oord, Bethge - 2015 - A note on the evaluation of generative models(3).pdf:pdf;:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Theis, Oord, Bethge - 2015 - A note on the evaluation of generative models.pdf:pdf},
issn = {00104841},
month = {nov},
number = {Nips},
title = {{A note on the evaluation of generative models}},
//url ={http://arxiv.org/abs/1705.10461 http://arxiv.org/abs/1511.01844},
year = {2015}
}
@article{Rakotomamonjy2018,
abstract = {This paper presents a distance-based discriminative framework for learning with probability distributions. Instead of using kernel mean embeddings or generalized radial basis kernels, we introduce embeddings based on dissimilarity of distributions to some reference distributions denoted as templates. Our framework extends the theory of similarity of Balcan et al. (2008) to the population distribution case and we show that, for some learning problems, some dissimilarity on distribution achieves low-error linear decision functions with high probability. Our key result is to prove that the theory also holds for empirical distributions. Algorithmically, the proposed approach consists in computing a mapping based on pairwise dissimilarity where learning a linear decision function is amenable. Our experimental results show that the Wasserstein distance embedding performs better than kernel mean embeddings and computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions.},
archivePrefix = {arXiv},
arxivId = {1803.00250},
author = {Rakotomamonjy, Alain and Traor{\'{e}}, Abraham and Berar, Maxime and Flamary, R{\'{e}}mi and Courty, Nicolas},
eprint = {1803.00250},
file = {:Users/hdeplaen/Desktop/1803.00250.pdf:pdf},
month = {mar},
number = {1},
title = {{Distance Measure Machines}},
//url ={http://arxiv.org/abs/1803.00250},
volume = {4108},
year = {2018}
}
@article{Chang2016,
author = {Chang, Xiaojun and Nie, Feiping and Yang, Y I and Zhang, Chengqi and Huang, Heng},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Chang et al. - 2016 - Convex Sparse PCA for Unsupervised Feature Learning.pdf:pdf},
number = {1},
pages = {1--16},
title = {{Convex Sparse PCA for Unsupervised Feature Learning}},
volume = {11},
year = {2016}
}
@inproceedings{Joachims:1999:TIT:645528.657646,
address = {San Francisco, CA, USA},
author = {Joachims, Thorsten},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
file = {:Users/hdeplaen/Desktop/joachims{\_}99c.pdf:pdf},
isbn = {1-55860-612-2},
pages = {200--209},
publisher = {Morgan Kaufmann Publishers Inc.},
series = {ICML '99},
title = {{Transductive Inference for Text Classification Using Support Vector Machines}},
//url ={http://dl.acm.org/citation.cfm?id=645528.657646},
year = {1999}
}
@article{Genevay2017,
abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
archivePrefix = {arXiv},
arxivId = {1706.00292},
author = {Genevay, Aude and Peyr{\'{e}}, Gabriel and Cuturi, Marco},
eprint = {1706.00292},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Genevay, Peyr{\'{e}}, Cuturi - 2017 - Learning Generative Models with Sinkhorn Divergences.pdf:pdf},
title = {{Learning Generative Models with Sinkhorn Divergences}},
//url ={http://arxiv.org/abs/1706.00292},
year = {2017}
}
@book{Vapnik1998,
address = {New York, NY},
author = {Vapnik, Vladimir N},
doi = {10.1007/978-1-4757-3264-1},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Vapnik - 2000 - The Nature of Statistical Learning Theory.pdf:pdf},
isbn = {978-1-4419-3160-3},
keywords = {imported},
publisher = {Springer New York},
title = {{The Nature of Statistical Learning Theory}},
//url ={http://link.springer.com/10.1007/978-1-4757-3264-1},
year = {2000}
}
@article{Suykens2017,
abstract = {Between years 2001 and 2004 a number of earthquakes with magnitudes which ranged from 3.9 to 5.0 occurred in the most seismically active areas of the Romanian territory. A macroseismic analysis of the effects produced on the Romanian territory has been conducted for these earthquakes, by using macroseismic questionnaires. Some of the observed intensities were significantly higher than those we could have expected after the earthquakes with such magnitudes. Effects have been evaluated from macroseismic observations giving maximum intensities estimated as VI–VII and VII in the MSK scale, respectively. Given the small to moderate size of earthquakes, many of the observed damages to buildings are due to their bad state, age, and poorly built without antiseismic protection, and hence particularly vulnerable.},
archivePrefix = {arXiv},
arxivId = {1706.02451},
author = {Suykens, Johan A. K.},
doi = {10.1162/neco_a_00984},
eprint = {1706.02451},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Suykens - 2017 - Deep Restricted Kernel Machines Using Conjugate Feature Duality.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Earthquake effects,Isoseismal map,Macroseismic questionnaires},
month = {aug},
number = {8},
pages = {2123--2163},
pmid = {25602775},
title = {{Deep Restricted Kernel Machines Using Conjugate Feature Duality}},
//url ={http://www.mitpressjournals.org/doi/abs/10.1162/neco{\_}a{\_}00984},
volume = {29},
year = {2017}
}
@article{Nagarajan2017,
archivePrefix = {arXiv},
arxivId = {1706.04156v3},
author = {Nagarajan, Vaishnavh},
eprint = {1706.04156v3},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Nagarajan - 2017 - Gradient descent GAN optimization is locally stable.pdf:pdf},
number = {Nips},
title = {{Gradient descent GAN optimization is locally stable}},
year = {2017}
}
@article{Bao2012,
abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
author = {Bao, Bing Kun and Liu, Guangcan and Xu, Changsheng and Yan, Shuicheng},
doi = {10.1109/TIP.2012.2192742},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Bao et al. - 2012 - Inductive robust principal component analysis.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error correction,robust principal component analysis (PCA),subspace learning},
number = {8},
pages = {3794--3800},
publisher = {IEEE},
title = {{Inductive robust principal component analysis}},
volume = {21},
year = {2012}
}
@article{Arora2014,
author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Arora et al. - 2014 - Generalization and Equilibrium in Generative Adversarial Nets ( GANs ).pdf:pdf},
title = {{Generalization and Equilibrium in Generative Adversarial Nets ( GANs )}},
year = {2014}
}
@article{Cuturi2013,
abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
archivePrefix = {arXiv},
arxivId = {1306.0895},
author = {Cuturi, Marco},
eprint = {1306.0895},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Optimal Transportation Distances.pdf:pdf},
isbn = {9781510810587},
issn = {10495258},
pages = {1--9},
pmid = {1714571},
title = {{Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances}},
//url ={http://arxiv.org/abs/1306.0895},
year = {2013}
}
@article{Mierswa,
author = {Mierswa, Ingo and Morik, Katharina},
file = {:Users/hdeplaen/Desktop/975f97d5893cb306c27bebcaf677ab28d966.pdf:pdf},
title = {{Learning with Non-Positive Semidefinite Kernels}}
}
@article{Snow2016,
abstract = {This paper focuses on a similarity measure, known as the Wasserstein distance, with which to compare images. The Wasserstein distance results from a partial differential equation (PDE) formulation of Monge's optimal transport problem. We present an efficient numerical solution method for solving Monge's problem. To demonstrate the measure's discriminatory power when comparing images, we use a {\$}1{\$}-Nearest Neighbour ({\$}1{\$}-NN) machine learning algorithm to illustrate the measure's potential benefits over other more traditional distance metrics and also the Tangent Space distance, designed to perform excellently on the well-known MNIST dataset. To our knowledge, the PDE formulation of the Wasserstein metric has not been presented for dealing with image comparison, nor has the Wasserstein distance been used within the {\$}1{\$}-nearest neighbour architecture.},
archivePrefix = {arXiv},
arxivId = {1612.00181},
author = {Snow, Michael and Van lent, Jan},
eprint = {1612.00181},
file = {:Users/hdeplaen/Desktop/1612.00181.pdf:pdf},
month = {dec},
title = {{Monge's Optimal Transport Distance for Image Classification}},
//url ={http://arxiv.org/abs/1612.00181},
year = {2016}
}
@article{Karevan,
author = {Karevan, Zahra and Feng, Yunlong and Suykens, Johan A. K.},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Karevan, Feng, Suykens - Unknown - Moving Least Squares Support Vector Machines for weather temperature prediction.pdf:pdf},
title = {{Moving Least Squares Support Vector Machines for weather temperature prediction}}
}
@article{Theis2015a,
abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
archivePrefix = {arXiv},
arxivId = {1511.01844},
author = {Theis, Lucas and van den Oord, A{\"{a}}ron and Bethge, Matthias},
eprint = {1511.01844},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Theis, Oord, Bethge - 2015 - A note on the evaluation of generative models(2).pdf:pdf},
pages = {1--10},
title = {{A note on the evaluation of generative models}},
//url ={http://arxiv.org/abs/1511.01844},
year = {2015}
}
@article{Coifman2006,
abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods. {\textcopyright} 2006.},
author = {Coifman, Ronald R. and Lafon, St{\'{e}}phane},
doi = {10.1016/j.acha.2006.04.006},
file = {:Users/hdeplaen/Desktop/Project Micha{\"{e}}l/1-s2.0-S1063520306000546-main.pdf:pdf},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {Diffusion metric,Diffusion processes,Dimensionality reduction,Eigenmaps,Graph Laplacian,Manifold learning},
number = {1},
pages = {5--30},
title = {{Diffusion maps}},
volume = {21},
year = {2006}
}
@book{Berg1984,
abstract = {The Fourier transform and the Laplace transform of a positive measure share, together with its moment sequence, a positive definiteness property which under certain regularity assumptions is characteristic for such expressions. This is formulated in exact terms in the famous theorems of Bochner, Bernstein-Widder and Hamburger. All three theorems can be viewed as special cases of a general theorem about functions qJ on abelian semigroups with involution (S, +, *) which are positive definite in the sense that the matrix (qJ(sJ + Sk» is positive definite for all finite choices of elements St, . . . , Sn from S. The three basic results mentioned above correspond to ({\~{}}, +, x* = -x), ([0, 00[, +, x* = x) and (No, +, n* = n). The purpose of this book is to provide a treatment of these positive definite functions on abelian semigroups with involution. In doing so we also discuss related topics such as negative definite functions, completely mono­ tone functions and Hoeffding-type inequalities. We view these subjects as important ingredients of harmonic analysis on semigroups. It has been our aim, simultaneously, to write a book which can serve as a textbook for an advanced graduate course, because we feel that the notion of positive definiteness is an important and basic notion which occurs in mathematics as often as the notion of a Hilbert space.},
address = {New York, NY},
author = {Berg, Christian and Christensen, Jens Peter Reus and Ressel, Paul},
doi = {10.1007/978-1-4612-1128-0},
isbn = {978-1-4612-7017-1},
publisher = {Springer New York},
series = {Graduate Texts in Mathematics},
title = {{Harmonic Analysis on Semigroups}},
//url ={http://link.springer.com/10.1007/978-1-4612-1128-0},
volume = {100},
year = {1984}
}
@article{Peyre2018,
abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
archivePrefix = {arXiv},
arxivId = {1803.00567},
author = {Peyr{\'{e}}, Gabriel and Cuturi, Marco},
eprint = {1803.00567},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Peyr{\'{e}}, Cuturi - 2018 - Computational Optimal Transport.pdf:pdf},
month = {mar},
title = {{Computational Optimal Transport}},
//url ={http://arxiv.org/abs/1803.00567},
year = {2018}
}
@inproceedings{Ong2004,
abstract = {What happens when we do not have a positive definite kernel? It turns out that we can still do learning, in what is now a Krein Space. This paper talks about regression with indefinite kernels.},
address = {New York, New York, USA},
author = {Ong, Cheng Soon and Mary, Xavier and Canu, St{\'{e}}phane and Smola, Alexander J},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015443},
file = {:Users/hdeplaen/Desktop/392.pdf:pdf},
isbn = {1581138285},
pages = {81},
publisher = {ACM Press},
title = {{Learning with non-positive kernels}},
//url ={http://portal.acm.org/citation.cfm?doid=1015330.1015443},
year = {2004}
}
@book{villani2008optimal,
abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results. PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book's value as a most welcome reference text on this subject.},
author = {Villani, C{\'{e}}dric},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Villani - 2008 - Optimal Transport Old and New.pdf:pdf},
isbn = {9783540710509},
keywords = {Optimal Transport,Wasserstein},
mendeley-tags = {Optimal Transport,Wasserstein},
publisher = {Springer Berlin Heidelberg},
series = {Grundlehren der mathematischen Wissenschaften},
title = {{Optimal Transport: Old and New}},
//url ={https://books.google.be/books?id=hV8o5R7{\_}5tkC},
year = {2008}
}
@article{Huang2017,
abstract = {Because of several successful applications, indefinite kernels have attracted many research interests in recent years. This paper addresses indefinite learning in the framework of least squares support vector machines (LS-SVM). Unlike C-SVM with an indefinite kernel, which involves a non-convex problem, the indefinite LS-SVM is still easy to solve, but the kernel trick and primal-dual relationship for LS-SVM with a Mercer kernel is no longer valid. In this paper, we give a feature space interpretation for indefinite LS-SVM. In the same framework, kernel principal component analysis with an infinite kernel is discussed as well. In numerical experiments, LS-SVM with an indefinite kernel, called the truncated l1 distance kernel, for classification and kernel principal component analysis is evaluated. Its good performance together with the feature space interpretation given in this paper imply the potential use of indefinite LS-SVM in real applications.},
author = {Huang, Xiaolin and Maier, Andreas and Hornegger, Joachim and Suykens, Johan A. K.},
doi = {10.1016/j.acha.2016.09.001},
file = {:Users/hdeplaen/Desktop/15{\_}214p.pdf:pdf},
issn = {1063-5203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {least squares support vector},
number = {1},
pages = {162--172},
publisher = {Elsevier Inc.},
title = {{Indefinite kernels in least squares support vector machines and principal component analysis}},
//url ={http://dx.doi.org/10.1016/j.acha.2016.09.001},
volume = {43},
year = {2017}
}
@article{Mehrkanoon2018,
author = {Mehrkanoon, Siamak and Huang, Xiaolin and Suykens, Johan A.K.},
doi = {10.1016/j.patcog.2018.01.014},
file = {:Users/hdeplaen/Desktop/17{\_}150.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Indefinite kernels,Kernel spectral clustering,Low embedding dimension,Scalable models,Semi-supervised learning,semi-supervised learning},
month = {jun},
pages = {144--153},
publisher = {Elsevier Ltd},
title = {{Indefinite kernel spectral learning}},
//url ={https://doi.org/10.1016/j.patcog.2018.01.014 https://linkinghub.elsevier.com/retrieve/pii/S003132031830013X},
volume = {78},
year = {2018}
}
@article{Genevay2018,
abstract = {Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on $\backslash$emph{\{}Sinkhorn divergences{\}} (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength {\$}\backslashvarepsilon{\$}, between OT ({\$}\backslashvarepsilon=0{\$}) and MMD ({\$}\backslashvarepsilon=\backslashinfty{\$}). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively {\$}O(n{\^{}}3\backslashlog n){\$}, {\$}O(n{\^{}}2){\$} and {\$}n{\^{}}2{\$} operations given a sample size {\$}n{\$}), much less is known in terms of their $\backslash$emph{\{}sample complexity{\}}, namely the gap between these quantities, when evaluated using finite samples $\backslash$emph{\{}vs.{\}} their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, {\$}1/n{\^{}}{\{}1/d{\}}{\$} for OT in dimension {\$}d{\$} and {\$}1/\backslashsqrt{\{}n{\}}{\$} for MMD, that for SDs has only been studied empirically. In this paper, we $\backslash$emph{\{}(i){\}} derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer {\$}\backslashvarepsilon{\$}, $\backslash$emph{\{}(ii){\}} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and $\backslash$emph{\{}(iii){\}} provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in {\$}1/\backslashsqrt{\{}n{\}}{\$} (as in MMD), with a constant that depends however on {\$}\backslashvarepsilon{\$}, making the bridge between OT and MMD complete.},
archivePrefix = {arXiv},
arxivId = {1810.02733},
author = {Genevay, Aude and Chizat, L{\'{e}}naic and Bach, Francis and Cuturi, Marco and Peyr{\'{e}}, Gabriel},
eprint = {1810.02733},
file = {:Users/hdeplaen/Desktop/1810.02733.pdf:pdf},
title = {{Sample Complexity of Sinkhorn divergences}},
//url ={http://arxiv.org/abs/1810.02733},
year = {2018}
}
@article{Mesa2018,
abstract = {The need to reason about uncertainty in large, complex, and multi-modal datasets has become increasingly common across modern scientific environments. The ability to transform samples from one distribution {\$}P{\$} to another distribution {\$}Q{\$} enables the solution to many problems in machine learning (e.g. Bayesian inference, generative modeling) and has been actively pursued from theoretical, computational, and application perspectives across the fields of information theory, computer science, and biology. Performing such transformations, in general, still leads to computational difficulties, especially in high dimensions. Here, we consider the problem of computing such "measure transport maps" with efficient and parallelizable methods. Under the mild assumptions that {\$}P{\$} need not be known but can be sampled from, and that the density of {\$}Q{\$} is known up to a proportionality constant, and that {\$}Q{\$} is log-concave, we provide in this work a convex optimization problem pertaining to relative entropy minimization. We show how an empirical minimization formulation and polynomial chaos map parameterization can allow for learning a transport map between {\$}P{\$} and {\$}Q{\$} with distributed and scalable methods. We also leverage findings from nonequilibrium thermodynamics to represent the transport map as a composition of simpler maps, each of which is learned sequentially with a transport cost regularized version of the aforementioned problem formulation. We provide examples of our framework within the context of Bayesian inference for the Boston housing dataset and generative modeling for handwritten digit images from the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1801.08454},
author = {Mesa, Diego A. and Tantiongloc, Justin and Mendoza, Marcela and Coleman, Todd P.},
eprint = {1801.08454},
file = {:Users/hdeplaen/Desktop/neco{\_}a{\_}01172.pdf:pdf},
keywords = {bayesian inference,convex optimization,generative modeling,machine learning,optimal transport theory,parallelized computation,relative entropy},
title = {{A Distributed Framework for the Construction of Transport Maps}},
//url ={http://arxiv.org/abs/1801.08454},
year = {2018}
}
@article{Solomon:2015:CWD:2809654.2766963,
address = {New York, NY, USA},
author = {Solomon, Justin and de Goes, Fernando and Peyr{\'{e}}, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
doi = {10.1145/2766963},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Solomon et al. - 2015 - Convolutional Wasserstein Distances Efficient Optimal Transportation on Geometric Domains.pdf:pdf},
issn = {0730-0301},
journal = {ACM Trans. Graph.},
keywords = {displacement interpolation,entropy,optimal transportation,wasserstein distances},
month = {jul},
number = {4},
pages = {66:1----66:11},
publisher = {ACM},
title = {{Convolutional Wasserstein Distances: Efficient Optimal Transportation on Geometric Domains}},
//url ={http://doi.acm.org/10.1145/2766963},
volume = {34},
year = {2015}
}
@article{Bigot2017,
abstract = {We introduce the method of Geodesic Principal Component Analysis (GPCA) on the space of probability measures on the line, with finite second moment, endowed with the Wasserstein metric. We discuss the advantages of this approach, over a standard functional PCA of probability densities in the Hilbert space of square-integrable functions. We establish the consistency of the method by showing that the empirical GPCA converges to its population counterpart, as the sample size tends to infinity. A key property in the study of GPCA is the isometry between the Wasserstein space and a closed convex subset of the space of square-integrable functions, with respect to an appropriate measure. Therefore, we consider the general problem of PCA in a closed convex subset of a separable Hilbert space, which serves as basis for the analysis of GPCA and also has interest in its own right. We provide illustrative examples on simple statistical models, to show the benefits of this approach for data analysis. The method is also applied to a real dataset of population pyramids.},
author = {Bigot, J{\'{e}}r{\'{e}}mie and Gouet, Ra{\'{u}}l and Klein, Thierry and L{\'{o}}pez, Alfredo},
doi = {10.1214/15-AIHP706},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Bigot et al. - 2017 - Geodesic PCA in the Wasserstein space by convex PCA.pdf:pdf},
issn = {0246-0203},
journal = {Annales de l'Institut Henri Poincar{\'{e}}, Probabilit{\'{e}}s et Statistiques},
keywords = {Fr{\'{e}}chet mean,Functional data analysis,Geodesic and Convex Principal Component Analysis,Geodesic space,Inference for family of densities,Wasserstein space},
month = {feb},
number = {1},
pages = {1--26},
title = {{Geodesic PCA in the Wasserstein space by convex PCA}},
//url ={http://projecteuclid.org/euclid.aihp/1486544882},
volume = {53},
year = {2017}
}
@inproceedings{Carriere2017,
author = {Carri\`{e}re, Mathieu and Cuturi, Marco and Oudot, Steve},
title = {Sliced Wasserstein Kernel for Persistence Diagrams},
year = {2017},
publisher = {JMLR.org},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {664–673},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML’17}
}

  


@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
doi = {10.2507/daaam.scibook.2010.27},
eprint = {1701.07875},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Chintala, Bottou - 2017 - Wasserstein GAN.pdf:pdf},
isbn = {9781845937591},
issn = {1701.07875},
month = {jan},
pmid = {19963286},
title = {{Wasserstein GAN}},
//url ={http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Mescheder2018,
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Mescheder, Geiger, Nowozin - 2018 - Which Training Methods for GANs do actually Converge.pdf:pdf},
title = {{Which Training Methods for GANs do actually Converge ?}},
year = {2018}
}
@article{JOZIAK2012,
archivePrefix = {arXiv},
arxivId = {1307.1778v2},
author = {J{\'{O}}ZIAK, PAWE{\L}},
doi = {https://doi.org/10.1080/03081087.2015.1015401},
eprint = {1307.1778v2},
file = {:Users/hdeplaen/Desktop/1307.1778.pdf:pdf},
keywords = {and phrases,conditionally negative definite kernels,conditionally strictly negative,coxeter groups,definite kernels,embedding into hilbert spaces},
pages = {1--12},
title = {{Conditionally strictly negative definite kernels}},
year = {2012}
}
@inproceedings{Suykens2003a,
abstract = {In this paper we present an extension of least squares support vector machines (LS-SVM's) to the multiclass case. While standard SVM solutions involve solving quadratic or linear programming problems, the least squares version of SVM's corresponds to solving a set of linear equations, due to equality instead of inequality constraints in the problem formulation. In LS-SVM's Mercer condition is still applicable. Hence several type of kernels such as polynomial, RBF's and MLP's can be used. The multiclass case that we discuss here is related to classical neural net approaches for classifi- cation where multi classes are encoded by considering multiple outputs for the network. Efficient methods for solving large scale LS-SVM's are available.},
author = {Suykens, Johan A. K. and Vandewalle, J.},
booktitle = {IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
doi = {10.1109/IJCNN.1999.831072},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Suykens, Vandewalle - 2003 - Multiclass least squares support vector machines.pdf:pdf},
isbn = {0-7803-5529-6},
number = {1},
pages = {900--903},
publisher = {IEEE},
title = {{Multiclass least squares support vector machines}},
//url ={http://ieeexplore.ieee.org/document/831072/},
volume = {2},
year = {2003}
}
@article{Lei2017,
abstract = {In this work, we show the intrinsic relations between optimal transportation and convex geometry, especially the variational approach to solve Alexandrov problem: constructing a convex polytope with prescribed face normals and volumes. This leads to a geometric interpretation to generative models, and leads to a novel framework for generative models. By using the optimal transportation view of GAN model, we show that the discriminator computes the Kantorovich potential, the generator calculates the transportation map. For a large class of transportation costs, the Kantorovich potential can give the optimal transportation map by a close-form formula. Therefore, it is sufficient to solely optimize the discriminator. This shows the adversarial competition can be avoided, and the computational architecture can be simplified. Preliminary experimental results show the geometric method outperforms WGAN for approximating probability measures with multiple clusters in low dimensional space.},
archivePrefix = {arXiv},
arxivId = {1710.05488},
author = {Lei, Na and Su, Kehua and Cui, Li and Yau, Shing-tung and Gu, David Xianfeng},
eprint = {1710.05488},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Lei et al. - 2017 - A Geometric View of Optimal Transportation and Generative Model(3).pdf:pdf},
month = {oct},
pages = {1--25},
title = {{A Geometric View of Optimal Transportation and Generative Model}},
//url ={http://arxiv.org/abs/1710.05488},
year = {2017}
}
@article{Memoli2011,
author = {M{\'{e}}moli, Facundo},
doi = {10.1016/j.acha.2010.09.005},
file = {:Users/hdeplaen/Desktop/1-s2.0-S1063520310001107-main.pdf:pdf},
issn = {1063-5203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {shape and data analysis},
number = {3},
pages = {363--401},
publisher = {Elsevier Inc.},
title = {{A spectral notion of Gromov – Wasserstein distance and related methods}},
//url ={http://dx.doi.org/10.1016/j.acha.2010.09.005},
volume = {30},
year = {2011}
}
@inproceedings{peyre:hal-01322992,
address = {New-York, United States},
author = {Peyr{\'{e}}, Gabriel and Cuturi, Marco and Solomon, Justin},
booktitle = {ICML 2016},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Peyr{\'{e}} et al. - 2016 - Gromov-Wasserstein Averaging of Kernel and Distance Matrices To cite this version HAL Id hal-01322992 Gromov-Wa.pdf:pdf},
keywords = {Gromov-Wasserstein,Optimal Transport,Wasserstein,metric spaces,shapes},
month = {jun},
series = {Proc. 33rd International Conference on Machine Learning},
title = {{Gromov-Wasserstein Averaging of Kernel and Distance Matrices}},
//url ={https://hal.archives-ouvertes.fr/hal-01322992},
year = {2016}
}
@book{villani2003topics,
abstract = {This is the first comprehensive introduction to the theory of mass transportation with its many - and sometimes unexpected - applications. In a novel approach to the subject, the book both surveys the topic and includes a chapter of problems, making it a particularly useful graduate textbook. In 1781, Gaspard Monge defined the problem of 'optimal transportation' (or the transferring of mass with the least possible amount of work), with applications to engineering in mind.In 1942, Leonid Kantorovich applied the newborn machinery of linear programming to Monge's problem, with applications to economics in mind. In 1987, Yann Brenier used optimal transportation to prove a new projection theorem on the set of measure preserving maps, with applications to fluid mechanics in mind. Each of these contributions marked the beginning of a whole mathematical theory, with many unexpected ramifications. Nowadays, the Monge-Kantorovich problem is used and studied by researchers from extremely diverse horizons, including probability theory, functional analysis, isoperimetry, partial differential equations, and even meteorology. Originating from a graduate course, the present volume is intended for graduate students and researchers, covering both theory and applications. Readers are only assumed to be familiar with the basics of measure theory and functional analysis.},
author = {Villani, C{\'{e}}dric},
file = {:Users/hdeplaen/Desktop/Cedric-Villani.pdf:pdf},
isbn = {9780821833124},
publisher = {American Mathematical Society},
series = {Graduate studies in mathematics},
title = {{Topics in Optimal Transportation}},
//url ={https://books.google.be/books?id=GqRXYFxe0l0C},
year = {2003}
}
@article{Xu2017,
abstract = {Let {\$}n \backslashin \backslashmathbb N{\$}, let {\$}\backslashzeta{\_}{\{}n,1{\}},...,\backslashzeta{\_}{\{}n,n{\}}{\$} be a sequence of independent random variables with {\$}\backslashmathbb E \backslashzeta{\_}{\{}n,i{\}}=0{\$} and {\$}\backslashmathbb E |\backslashzeta{\_}{\{}n,i{\}}|{\textless}\backslashinfty{\$} for each {\$}i{\$}, and let {\$}\backslashmu{\$} be an {\$}\backslashalpha{\$}-stable distribution having characteristic function {\$}e{\^{}}{\{}-|\backslashlambda|{\^{}}{\{}\backslashalpha{\}}{\}}{\$} with {\$}\backslashalpha\backslashin (1,2){\$}. Denote {\$}S{\_}{\{}n{\}}=\backslashzeta{\_}{\{}n,1{\}}+...+\backslashzeta{\_}{\{}n,n{\}}{\$} and its distribution by {\$}\backslashmathcal L(S{\_}n){\$}, we bound the Wasserstein distance of {\$}\backslashmathcal L(S{\_}{\{}n{\}}){\$} and {\$}\backslashmu{\$} essentially by an {\$}L{\^{}}{\{}1{\}}{\$} discrepancy between two kernels, this bound can be interpreted as a generalization of the Stein discrepancy (in {\$}L{\^{}}{\{}2{\}}{\$} sense) introduced by Ledoux, Nourdin and Peccati. More precisely, we prove the following inequality: $\backslash$begin{\{}equation{\}} $\backslash$begin{\{}split{\}} d{\_}W$\backslash$left($\backslash$mathcal L (S{\_}n), $\backslash$mu$\backslash$right) $\backslash$ $\backslash$le C $\backslash$left[$\backslash$sum{\_}{\{}i=1{\}}{\^{}}n$\backslash$int{\_}{\{}-N{\}}{\^{}}N $\backslash$left|$\backslash$frac{\{}$\backslash$mathcal K{\_}$\backslash$alpha(t,N){\}}n -$\backslash$frac{\{} K{\_}i(t,N){\}}{\{}$\backslash$alpha{\}}$\backslash$right| d t $\backslash$ +$\backslash$ $\backslash$mathcal R{\_}{\{}N,n{\}}$\backslash$right], $\backslash$end{\{}split{\}} $\backslash$end{\{}equation{\}} where {\$}d{\_}{\{}W{\}}{\$} is the Wasserstein distance of probability measures, {\$}\backslashmathcal K{\_}\backslashalpha(t,N){\$} is the kernel of a decomposition of the fractional Laplacian {\$}\backslashDelta{\^{}}{\{}\backslashfrac \backslashalpha2{\}}{\$}, {\$} K{\_}i(t,N){\$} is a kernel introduced by Chen, Goldstein and Shao with a truncation which can be interpreted as an {\$}L{\^{}}1{\$} Stein kernel, and {\$}\backslashmathcal R{\_}{\{}N,n{\}}{\$} is a small remainder. The integral term {\$}{\$}$\backslash$sum{\_}{\{}i=1{\}}{\^{}}n$\backslash$int{\_}{\{}-N{\}}{\^{}}N $\backslash$left|$\backslash$frac{\{}$\backslash$mathcal K{\_}$\backslash$alpha(t,N){\}}n -$\backslash$frac{\{} K{\_}i(t,N){\}}{\{}$\backslash$alpha{\}}$\backslash$right| d t{\$}{\$} can be interpreted as an {\$}L{\^{}}{\{}1{\}}{\$} Stein discrepancy. As an application, we prove a general theorem of stable law convergence rate when {\$}\backslashzeta{\_}{\{}n,i{\}}{\$} are i.i.d. and the distribution falls in the normal domain of attraction of {\$}\backslashmu{\$}. We also study four examples with comparing our convergence rates and those known for these examples, among which the distribution in the second example is not in the normal domain of attraction of {\$}\backslashmu{\$}.},
archivePrefix = {arXiv},
arxivId = {1709.00805},
author = {Xu, Lihu},
eprint = {1709.00805},
file = {:Users/hdeplaen/Desktop/1709.00805.pdf:pdf},
keywords = {discrepancy,l 1,normal domain of attraction,of stable law,s method,stable approximation,stein,w 1 distance,wasserstein-1 distance,$\alpha$ -stable processes},
month = {sep},
title = {{Approximation of stable law in Wasserstein-1 distance by Stein's method}},
//url ={http://arxiv.org/abs/1709.00805},
year = {2017}
}
@incollection{Houthuys2018,
abstract = {Several computational models have been designed to help our understanding of the conditions under which persistent activity can be sustained in cortical circuits during working memory tasks. Here we focus on one such model that has shown promise, that uses polychronization and short term synaptic dynamics to achieve this reverberation, and explore it with respect to different physiological parameters in the brain, including size of the network, number of synaptic connections, small-world connectivity, maximum axonal conduction delays, and type of cells (excitatory or inhibitory). We show that excitation and axonal conduction delays greatly affect the sustainability of spatio-temporal patterns of spikes called polychronous groups. {\textcopyright} 2014 Springer International Publishing Switzerland.},
archivePrefix = {arXiv},
arxivId = {1412.7927},
author = {Houthuys, Lynn and Suykens, Johan A. K.},
doi = {10.1007/978-3-030-01421-6_21},
eprint = {1412.7927},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Houthuys, Suykens - 2018 - Tensor Learning in Multi-view Kernel PCA.pdf:pdf},
isbn = {978-3-642-40727-7},
issn = {16113349},
keywords = {Kernel PCA,Multi-view learning,Tensor learning,kernel pca,multi-view learning,tensor learning},
pages = {205--215},
pmid = {23964431},
publisher = {Springer International Publishing},
title = {{Tensor Learning in Multi-view Kernel PCA}},
//url ={http://link.springer.com/10.1007/978-3-642-40728-4 http://link.springer.com/10.1007/978-3-030-01421-6{\_}21},
volume = {8131},
year = {2018}
}
@article{Montavon2016,
author = {Montavon, Gr{\'{e}}goire and Cuturi, Marco and Paris-saclay, Universit{\'{e}}},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Montavon, Cuturi, Paris-saclay - 2016 - Wasserstein Training of Restricted Boltzmann Machines.pdf:pdf},
journal = {Nips},
number = {1},
pages = {1--9},
title = {{Wasserstein Training of Restricted Boltzmann Machines}},
year = {2016}
}
@book{murphy2013machine,
address = {Cambridge, Mass. [u.a.]},
author = {Murphy, Kevin P},
file = {:Users/hdeplaen/Library/Application Support/Mendeley Desktop/Downloaded/Borovcnik, Bentz, Kapadia - 1991 - Machine Learning A Probabilistic Perspective.pdf:pdf},
isbn = {9780262018029 0262018020},
keywords = {hmm lda learning machine statistics},
publisher = {MIT Press},
title = {{Machine learning : a probabilistic perspective}},
//url ={https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr{\_}1{\_}2?ie=UTF8{\&}qid=1336857747{\&}sr=8-2},
year = {2013}
}
@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  //url ={http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}
@book{GabrielPeyre2019COTW,
publisher = {Now Foundations and Trends},
year = {2019},
title = {Computational Optimal Transport: With Applications to Data Science},
author = {Gabriel Peyr\'e and Marco Cuturi},
}
@book{Scholkopf:2001:LKS:559923,
 author = {Scholkopf, Bernhard and Smola, Alexander J.},
 title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
 year = {2001},
 isbn = {0262194759},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{mlssvm,
author={Johan A. K. {Suykens} and Joos {Vandewalle}},
booktitle={IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
title={Multiclass least squares support vector machines},
year={1999},
volume={2},
number={},
pages={900-903 vol.2},
keywords={pattern classification;radial basis function networks;multilayer perceptrons;learning (artificial intelligence);least squares approximations;least squares;support vector machines;linear equations;Mercer condition;multiclass case;RBF neural nets;pattern classification;multilayer perceptrons;learning sets;Least squares methods;Support vector machines;Support vector machine classification;Kernel;Polynomials;Neural networks;Large-scale systems;Quadratic programming;Linear programming;Equations},
doi={10.1109/IJCNN.1999.831072},
ISSN={1098-7576},
month={7},}

@incollection{Altschuler2017,
title = {Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},
author = {Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},
booktitle = {Advances in Neural Information Processing Systems 30},
//editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1964--1974},
year = {2017},
publisher = {Curran Associates, Inc.},
//url = {http://papers.nips.cc/paper/6792-near-linear-time-approximation-algorithms-for-optimal-transport-via-sinkhorn-iteration.pdf}
}


@inproceedings{girshick2015fast,
  title={Fast R-CNN},
  author={Girshick, Ross},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015},
  pages={1440--1448},
}



@inproceedings{he2017maskrcnn,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@inproceedings{redmon2016yolo,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@inproceedings{liu2016ssd,
  title={Ssd: Single shot multibox detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle={European conference on computer vision},
  pages={21--37},
  year={2016},
  organization={Springer}
}

@inproceedings{carion2020detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}


@inproceedings{
zhu2020deformabledetr,
title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},
author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=gZ9hCDWe6ke}
}

@inproceedings{li2022dndetr,
  title={Dn-detr: Accelerate detr training by introducing query denoising},
  author={Li, Feng and Zhang, Hao and Liu, Shilong and Guo, Jian and Ni, Lionel M and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13619--13627},
  year={2022}
}

@inproceedings{liu2021dabdetr,
  title={DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR},
  author={Liu, Shilong and Li, Feng and Zhang, Hao and Yang, Xiao and Qi, Xianbiao and Su, Hang and Zhu, Jun and Zhang, Lei},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{zhang2022dinodetr,
  title={Dino: Detr with improved denoising anchor boxes for end-to-end object detection},
  author={Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2203.03605},
  year={2022}
}

@inproceedings{lin2017focalloss,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@article{munkres1957algorithmstransportationhungarian,
  title={Algorithms for the assignment and transportation problems},
  author={Munkres, James},
  journal={Journal of the society for industrial and applied mathematics},
  volume={5},
  number={1},
  pages={32--38},
  year={1957},
  publisher={SIAM}
}

@article{liu2020objectdetectionsurvey,
  title={Deep learning for generic object detection: A survey},
  author={Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"a}inen, Matti},
  journal={International journal of computer vision},
  volume={128},
  number={2},
  pages={261--318},
  year={2020},
  publisher={Springer}
}

@inproceedings{girshick2014rcnn,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@article{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{knight2013fastbalancing,
  title={A fast algorithm for matrix balancing},
  author={Knight, Philip A and Ruiz, Daniel},
  journal={IMA Journal of Numerical Analysis},
  volume={33},
  number={3},
  pages={1029--1047},
  year={2013},
  publisher={Oxford University Press}
}

@article{frogner2015learningwasserstein,
  title={Learning with a Wasserstein loss},
  author={Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya, Mauricio and Poggio, Tomaso A},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{chizat2018unbalanced,
  title={Unbalanced optimal transport: Dynamic and Kantorovich formulations},
  author={Chizat, Lenaic and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c{c}}ois-Xavier},
  journal={Journal of Functional Analysis},
  volume={274},
  number={11},
  pages={3090--3123},
  year={2018},
  publisher={Elsevier}
}

@article{chizat2018scaling,
  title={Scaling algorithms for unbalanced optimal transport problems},
  author={Chizat, Lenaic and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c{c}}ois-Xavier},
  journal={Mathematics of Computation},
  volume={87},
  number={314},
  pages={2563--2609},
  year={2018}
}

@article{liero2018optimal,
  title={Optimal entropy-transport problems and a new Hellinger--Kantorovich distance between positive measures},
  author={Liero, Matthias and Mielke, Alexander and Savar{\'e}, Giuseppe},
  journal={Inventiones mathematicae},
  volume={211},
  number={3},
  pages={969--1117},
  year={2018},
  publisher={Springer}
}

@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}


@article{monge1781memoire,
  title={M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
  author={Monge, Gaspard},
  journal={Mem. Math. Phys. Acad. Royale Sci.},
  pages={666--704},
  year={1781}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{buckland1994precisionrecall,
  title={The relationship between recall and precision},
  author={Buckland, Michael and Gey, Fredric},
  journal={Journal of the American society for information science},
  volume={45},
  number={1},
  pages={12--19},
  year={1994},
  publisher={Wiley Online Library}
}

@inproceedings{fatras2021unbalanced,
  title={Unbalanced minibatch optimal transport; applications to domain adaptation},
  author={Fatras, Kilian and S{\'e}journ{\'e}, Thibault and Flamary, R{\'e}mi and Courty, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={3186--3197},
  year={2021},
  organization={PMLR}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HENRI

@book{brualdi_2006, 
place={Cambridge}, 
series={Encyclopedia of Mathematics and its Applications}, 
title={Combinatorial Matrix Classes}, 
DOI={10.1017/CBO9780511721182}, 
publisher={Cambridge University Press}, 
author={Brualdi, Richard A.}, 
year={2006}, 
collection={Encyclopedia of Mathematics and its Applications}
}

@INPROCEEDINGS{giou,
author = {H. Rezatofighi and N. Tsoi and J. Gwak and A. Sadeghian and I. Reid and S. Savarese},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression},
year = {2019},
volume = {},
issn = {},
pages = {658-666},
abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
keywords = {},
doi = {10.1109/CVPR.2019.00075},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00075},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@book{santambrogio,
  title={Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and Modeling},
  author={Santambrogio, F.},
  isbn={9783319208282},
  series={Progress in Nonlinear Differential Equations and Their Applications},
  url={https://books.google.be/books?id=UOHHCgAAQBAJ},
  year={2015},
  publisher={Springer International Publishing}
}



% ot papers
@article{kantorovich,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2626967},
 abstract = {The following paper is reproduced from a Russian journal of the character of our own Proceedings of the National Academy of Sciences, Comptes Rendus (Doklady) de l'Académie des Sciences de l'URSS, 1942, Volume XXXVII, No. 7-8. The author is one of the most distinguished of Russian mathematicians. He has made very important contributions in pure mathematics in the theory of functional analysis, and has made equally important contributions to applied mathematics in numerical analysis and the theory and practice of computation. Although his exposition in this paper is quite terse and couched in mathematical language which may be difficult for some readers of Management Science to follow, it is thought that this presentation will: (1) make available to American readers generally an important work in the field of linear programming, (2) provide an indication of the type of analytic work which has been done and is being done in connection with rational planning in Russia, (3) through the specific examples mentioned indicate the types of interpretation which the Russians have made of the abstract mathematics (for example, the potential and field interpretations adduced in this country recently by W. Prager were anticipated in this paper).},
 author = {L. Kantorovitch},
 journal = {Management Science},
 number = {1},
 pages = {1--4},
 publisher = {INFORMS},
 title = {On the Translocation of Masses},
 urldate = {2022-11-03},
 volume = {5},
 year = {1958}
}



@inproceedings{lightspeed,
 author = {Cuturi, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 url = {https://proceedings.neurips.cc/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{schmitzer2019stabilized,
  title={Stabilized sparse scaling algorithms for entropy regularized transport problems},
  author={Schmitzer, Bernhard},
  journal={SIAM Journal on Scientific Computing},
  volume={41},
  number={3},
  pages={A1443--A1481},
  year={2019},
  publisher={SIAM}
}

@inproceedings{greenkorn,
 author = {Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},
 url = {https://proceedings.neurips.cc/paper/2017/file/491442df5f88c6aa018e86dac21d3606-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{screenkorn,
  title={Screening sinkhorn algorithm for regularized optimal transport},
  author={Alaya, Mokhtar Z and Berar, Maxime and Gasso, Gilles and Rakotomamonjy, Alain},
  booktitle = {Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}



@InProceedings{sinkhorn-complexity,
  title = 	 {Sample Complexity of Sinkhorn Divergences},
  author =       {Genevay, Aude and Chizat, L\'{e}na\"{i}c and Bach, Francis and Cuturi, Marco and Peyr\'{e}, Gabriel},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1574--1583},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/genevay19a/genevay19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/genevay19a.html},
  abstract = 	 {Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures.  We focus in this paper on Sinkhorn divergences (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength $\varepsilon$, between OT ($\varepsilon=0$) and MMD ($\varepsilon=\infty$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively $O(n^3\log n)$, $O(n^2)$ and $n^2$ operations given a sample size $n$), much less is known in terms of their sample complexity, namely the gap between these quantities, when evaluated using finite samples vs. their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, $1/n^{1/d}$ for OT in dimension $d$ and $1/\sqrt{n}$ for MMD, that for SDs has only been studied empirically. In this paper, we (i) derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer $\varepsilon$, (ii) prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and (iii) provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in $1/\sqrt{n}$ (as in MMD), with a constant that depends however on $\varepsilon$, making the bridge between OT and MMD complete.}
}

@article{CHIZAT20183090,
title = {Unbalanced optimal transport: Dynamic and Kantorovich formulations},
journal = {Journal of Functional Analysis},
volume = {274},
number = {11},
pages = {3090-3123},
issn = {0022-1236},
doi = {https://doi.org/10.1016/j.jfa.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022123618301058},
author = {Lénaïc Chizat and Gabriel Peyré and Bernhard Schmitzer and François-Xavier Vialard},
keywords = {Unbalanced optimal transport},
abstract = {This article presents a new class of distances between arbitrary nonnegative Radon measures inspired by optimal transport. These distances are defined by two equivalent alternative formulations: (i) a dynamic formulation defining the distance as a geodesic distance over the space of measures (ii) a static “Kantorovich” formulation where the distance is the minimum of an optimization problem over pairs of couplings describing the transfer (transport, creation and destruction) of mass between two measures. Both formulations are convex optimization problems, and the ability to switch from one to the other depending on the targeted application is a crucial property of our models. Of particular interest is the Wasserstein–Fisher–Rao metric recently introduced independently by [7], [15]. Defined initially through a dynamic formulation, it belongs to this class of metrics and hence automatically benefits from a static Kantorovich formulation.}
}

@phdthesis{chizat-these,
  TITLE = {{Unbalanced Optimal Transport : Models, Numerical Methods, Applications}},
  AUTHOR = {Chizat, Lenaic},
  URL = {https://tel.archives-ouvertes.fr/tel-01881166},
  NUMBER = {2017PSLED063},
  SCHOOL = {{Universit{\'e} Paris sciences et lettres}},
  YEAR = {2017},
  MONTH = Nov,
  KEYWORDS = {Optimal transport ; Convex analysis ; Optimization ; Nonnegative measures ; Information geometry ; Sinkhorn's algorithm ; Image processing ; Gradient flows ; Weak convergence ; Tensor field processing ; Barycentres ; Relative entropy ; Geodesic metric space ; Transport optimal ; Analyse convexe ; Optimisation ; Mesures positives ; G{\'e}om{\'e}trie de l'information ; Algorithme de Sinkhorn ; Traitement d'image ; Flots de gradient ; Convergence faible ; Traitement de champs de tenseurs ; Barycentres ; Entropie relative ; Espace m{\'e}trique g{\'e}od{\'e}sique},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-01881166/file/These-Finale-CHIZAT.pdf},
  HAL_ID = {tel-01881166},
  HAL_VERSION = {v1},
}


@InProceedings{genevay,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Genevay, Aude and Peyre, Gabriel and Cuturi, Marco},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}

@phdthesis{genevay-these,
  TITLE = {{Entropy-Regularized Optimal Transport for Machine Learning}},
  AUTHOR = {Genevay, Aude},
  URL = {https://tel.archives-ouvertes.fr/tel-02319318},
  SCHOOL = {{PSL University}},
  YEAR = {2019},
  MONTH = Mar,
  KEYWORDS = {Optimal transport ; Machine learning ; Transport optimal ; Apprentissage statistique},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-02319318/file/these_aude.pdf},
  HAL_ID = {tel-02319318},
  HAL_VERSION = {v1},
}

@article{wass-gaussians,
title = {The Fréchet distance between multivariate normal distributions},
journal = {Journal of Multivariate Analysis},
volume = {12},
number = {3},
pages = {450-455},
year = {1982},
issn = {0047-259X},
doi = {https://doi.org/10.1016/0047-259X(82)90077-X},
url = {https://www.sciencedirect.com/science/article/pii/0047259X8290077X},
author = {D.C Dowson and B.V Landau},
keywords = {Fréchet distance, multivariate normal distributions, covariance matrices},
abstract = {The Fréchet distance between two multivariate normal distributions having means μX, μY and covariance matrices ΣX, ΣY is shown to be given by d2 = |μX − μY|2 + tr(ΣX + ΣY − 2(ΣXΣY)12). The quantity d0 given by d02 = tr(ΣX + ΣY − 2(ΣXΣY)12) is a natural metric on the space of real covariance matrices of given order.}
}

@InProceedings{rotated-gaussians,
  title = 	 {Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss},
  author =       {Yang, Xue and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng and Tian, Qi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11830--11841},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21l/yang21l.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21l.html},
  abstract = 	 {Boundary discontinuity and its inconsistency to the final detection metric have been the bottleneck for rotating detection regression loss design. In this paper, we propose a novel regression loss based on Gaussian Wasserstein distance as a fundamental approach to solve the problem. Specifically, the rotated bounding box is converted to a 2-D Gaussian distribution, which enables to approximate the indifferentiable rotational IoU induced loss by the Gaussian Wasserstein distance (GWD) which can be learned efficiently by gradient back-propagation. GWD can still be informative for learning even there is no overlapping between two rotating bounding boxes which is often the case for small object detection. Thanks to its three unique properties, GWD can also elegantly solve the boundary discontinuity and square-like problem regardless how the bounding box is defined. Experiments on five datasets using different detectors show the effectiveness of our approach, and codes are available at https://github.com/yangxue0827/RotationDetection.}
}

@article{mixture-wasserstein,
author = {Delon, Julie and Desolneux, Agn\`{e}s},
title = {A Wasserstein-Type Distance in the Space of Gaussian Mixture Models},
journal = {SIAM Journal on Imaging Sciences},
volume = {13},
number = {2},
pages = {936-970},
year = {2020},
doi = {10.1137/19M1301047},
URL = { https://doi.org/10.1137/19M1301047},
eprint = {https://doi.org/10.1137/19M1301047},
abstract = { In this paper we introduce a Wasserstein-type distance on the set of Gaussian mixture models. This distance is defined by restricting the set of possible coupling measures in the optimal transport problem to Gaussian mixture models. We derive a very simple discrete formulation for this distance, which makes it suitable for high dimensional problems. We also study the corresponding multi-marginal and barycenter formulations. We show some properties of this Wasserstein-type distance, and we illustrate its practical use with some examples in image processing.}
}

@article{hungarian-cubic,
author = {Edmonds, Jack and Karp, Richard M.},
title = {Theoretical Improvements in Algorithmic Efficiency for Network Flow Problems},
year = {1972},
issue_date = {April 1972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321694.321699},
doi = {10.1145/321694.321699},
journal = {J. ACM},
month = {apr},
pages = {248–264},
numpages = {17}
}

@inproceedings{sinkhorn-divergences,
author = {Chizat, L\'{e}na\"{\i}c and Roussillon, Pierre and L\'{e}ger, Flavien and Vialard, Fran\c{c}ois-Xavier and Peyr\'{e}, Gabriel},
title = {Faster Wasserstein Distance Estimation with the Sinkhorn Divergence},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The squared Wasserstein distance is a natural quantity to compare probability distributions in a non-parametric setting. This quantity is usually estimated with the plug-in estimator, defined via a discrete optimal transport problem which can be solved to ε-accuracy by adding an entropic regularization of order ε and using for instance Sinkhorn's algorithm. In this work, we propose instead to estimate it with the Sinkhorn divergence, which is also built on entropic regularization but includes debiasing terms. We show that, for smooth densities, this estimator has a comparable sample complexity but allows higher regularization levels, of order ε1/2, which leads to improved computational complexity bounds and a strong speedup in practice. Our theoretical analysis covers the case of both randomly sampled densities and deterministic discretizations on uniform grids. We also propose and analyze an estimator based on Richardson extrapolation of the Sinkhorn divergence which enjoys improved statistical and computational efficiency guarantees, under a condition on the regularity of the approximation error, which is in particular satisfied for Gaussian densities. We finally demonstrate the efficiency of the proposed estimators with numerical experiments.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {190},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}







%%% LAP GPU
@article{gpu-hungarian,
title = {GPU-accelerated Hungarian algorithms for the Linear Assignment Problem},
journal = {Parallel Computing},
volume = {57},
pages = {52-72},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2016.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S016781911630045X},
author = {Ketan Date and Rakesh Nagi},
keywords = {Linear assignment problem, Parallel algorithm, Graphics processing unit, CUDA},
abstract = {In this paper, we describe parallel versions of two different variants (classical and alternating tree) of the Hungarian algorithm for solving the Linear Assignment Problem (LAP). We have chosen Compute Unified Device Architecture (CUDA) enabled NVIDIA Graphics Processing Units (GPU) as the parallel programming architecture because of its ability to perform intense computations on arrays and matrices. The main contribution of this paper is an efficient parallelization of the augmenting path search phase of the Hungarian algorithm. Computational experiments on problems with up to 25 million variables reveal that the GPU-accelerated versions are extremely efficient in solving large problems, as compared to their CPU counterparts. Tremendous parallel speedups are achieved for problems with up to 400 million variables, which are solved within 13 seconds on average. We also tested multi-GPU versions of the two variants on up to 16 GPUs, which show decent scaling behavior for problems with up to 1.6 billion variables and dense cost matrix structure.}
}

@inproceedings{gpu-bipartite,
  title={Bipartite graph matching computation on GPU},
  author={Vasconcelos, Cristina Nader and Rosenhahn, Bodo},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={42--55},
  year={2009},
  organization={Springer}
}

@incollection{gpu-matching,
  title={A GPU algorithm for greedy graph matching},
  author={Fagginger Auer, Bas O and Bisseling, Rob H},
  booktitle={Facing the Multicore-Challenge II},
  pages={108--119},
  year={2012},
  publisher={Springer}
}
















%% MORE MODELS
@inproceedings{pang2019libra,
  title={Libra r-cnn: Towards balanced learning for object detection},
  author={Pang, Jiangmiao and Chen, Kai and Shi, Jianping and Feng, Huajun and Ouyang, Wanli and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={821--830},
  year={2019}
}

@inproceedings{cai2018cascade,
  title={Cascade r-cnn: Delving into high quality object detection},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6154--6162},
  year={2018}
}

@article{cai2019cascade,
  title={Cascade R-CNN: high quality object detection and instance segmentation},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={5},
  pages={1483--1498},
  year={2019},
  publisher={IEEE}
}

@article{dai2016r,
  title={R-FCN: Object detection via region-based fully convolutional networks},
  author={Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2117--2125},
  year={2017}
}

@article{he2015spatial,
  title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={9},
  pages={1904--1916},
  year={2015},
  publisher={IEEE}
}



@inproceedings{de2020wasserstein,
  title={Wasserstein exponential kernels},
  author={De Plaen, Henri and Fanuel, Micha{\"e}l and Suykens, Johan AK},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}


@article{montavon2016wasserstein,
  title={Wasserstein training of restricted Boltzmann machines},
  author={Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Cuturi, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}


@inproceedings{tolstikhin2017wasserstein,
  title={Wasserstein Auto-Encoders},
  author={Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{kolouri2018sliced,
  title={Sliced Wasserstein auto-encoders},
  author={Kolouri, Soheil and Pope, Phillip E and Martin, Charles E and Rohde, Gustavo K},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{rubenstein2018latent,
  title={On the latent space of wasserstein auto-encoders},
  author={Rubenstein, Paul K and Schoelkopf, Bernhard and Tolstikhin, Ilya},
  journal={arXiv preprint arXiv:1802.03761},
  year={2018}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

 @InProceedings{Han_2020_CVPR_Workshops,
author = {Han, Yuzhuo and Liu, Xiaofeng and Sheng, Zhenfei and Ren, Yutao and Han, Xu and You, Jane and Liu, Risheng and Luo, Zhongxuan},
title = {Wasserstein Loss-Based Deep Object Detection},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}


@InProceedings{pmlr-v139-yang21l,
  title = 	 {Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss},
  author =       {Yang, Xue and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng and Tian, Qi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11830--11841},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21l/yang21l.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21l.html}
}



@inproceedings{kolouri2016sliced,
  title={Sliced Wasserstein kernels for probability distributions},
  author={Kolouri, Soheil and Zou, Yang and Rohde, Gustavo K},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5258--5267},
  year={2016}
}


%% OT and Object detection
@inproceedings{ge2021ota,
  title={Ota: Optimal transport assignment for object detection},
  author={Ge, Zheng and Liu, Songtao and Li, Zeming and Yoshie, Osamu and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={303--312},
  year={2021}
}

@article{wang2021normalized,
  title={A normalized Gaussian Wasserstein distance for tiny object detection},
  author={Wang, Jinwang and Xu, Chang and Yang, Wen and Yu, Lei},
  journal={arXiv preprint arXiv:2110.13389},
  year={2021}
}

@inproceedings{otani2022optimal,
  title={Optimal Correction Cost for Object Detection Evaluation},
  author={Otani, Mayu and Togashi, Riku and Nakashima, Yuta and Rahtu, Esa and Heikkil{\"a}, Janne and Satoh, Shin'ichi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21107--21115},
  year={2022}
}

@article{chen2022group,
  title={Group DETR: Fast Training Convergence with Decoupled One-to-Many Label Assignment},
  author={Chen, Qiang and Chen, Xiaokang and Zeng, Gang and Wang, Jingdong},
  journal={arXiv preprint arXiv:2207.13085},
  year={2022}
}


@article{vo2022review,
  title={A review on anchor assignment and sampling heuristics in deep learning-based object detection},
  author={Vo, Xuan-Thuy and Jo, Kang-Hyun},
  journal={Neurocomputing},
  year={2022},
  publisher={Elsevier}
}

@article{ge2021yolox,
  title={Yolox: Exceeding yolo series in 2021},
  author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
  journal={arXiv preprint arXiv:2107.08430},
  year={2021}
}

@ARTICLE{9152115,  author={Lee, John and Bertrand, Nicholas P. and Rozell, Christopher J.},  journal={IEEE Transactions on Computational Imaging},   title={Unbalanced Optimal Transport Regularization for Imaging Problems},   year={2020},  volume={6},  number={},  pages={1219-1232},  doi={10.1109/TCI.2020.3012954}}

%% FROM REBUTTAL
@inproceedings{loftr,
  title={LoFTR: Detector-free local feature matching with transformers},
  author={Sun, Jiaming and Shen, Zehong and Wang, Yuang and Bao, Hujun and Zhou, Xiaowei},
  booktitle={
  CVPR},
  pages={8922--8931},
  year={2021}
}

@inproceedings{ren2015fasterrcnn,
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume={28},
  year={2015}
}

@article{YangPilanci,
	title        = {RANDOMIZED SKETCHES FOR KERNELS: FAST AND OPTIMAL NONPARAMETRIC REGRESSION},
	author       = {Yun Yang and Mert Pilanci and Martin J. Wainwright},
	year         = 2017,
	journal      = {The Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 45,
	number       = 3,
	pages        = {991--1023}
}
@article{suykensdeep2017,
	title        = {Deep Restricted Kernel Machines using Conjugate Feature Duality},
	author       = {Suykens, Johan A. K.},
	year         = 2017,
	journal      = {Neural Computation},
	volume       = 29,
	number       = 8,
	pages        = {2123--2163},
	issn         = {0899-7667, 1530-888X},
	language     = {en}
}
@inproceedings{Karras_2019_CVPR,
	title        = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	author       = {Karras, Tero and Laine, Samuli and Aila, Timo},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{Woodruff,
	title        = {Subspace Embeddings for the Polynomial Kernel},
	author       = {Avron, Haim and Nguyen, Huy and Woodruff, David},
	year         = 2014,
	booktitle    = {Advances in Neural Information Processing Systems},
	volume       = 27,
	pages        = {2258--2266}
}
@book{AbsilBook,
	title        = {Optimization Algorithms on Matrix Manifolds},
	author       = {P.-A. Absil and R. Mahony and R. Sepulchre},
	year         = 2008,
	publisher    = {Princeton University Press},
	address      = {Princeton, NJ},
	pages        = {xvi+224},
	isbn         = {978-0-691-13298-3},
	keywords     = {optimization on manifolds, Riemannian optimization, retraction, vector transport}
}
@book{Nesterov,
	title        = {Introductory Lectures on Convex Optimization: A Basic Course},
	author       = {Nesterov, Yurii},
	year         = 2014,
	publisher    = {Springer Publishing Company, Incorporated},
	isbn         = 1461346916,
	edition      = {1st}
}
@inproceedings{MIG_VAE,
	title        = {Isolating Sources of Disentanglement in Variational Autoencoders},
	author       = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems 31},
	pages        = {2610--2620}
}
@inproceedings{FactorVAE,
	title        = {Disentangling by Factorising},
	author       = {Kim, Hyunjik and Mnih, Andriy},
	year         = 2018,
	booktitle    = {proceedings of the Thirty-fifth International Conference on Machine Learning (ICML)},
	volume       = 80,
	pages        = {2649--2658}
}
@misc{lecun-mnisthandwrittendigit2010,
	title        = {MNIST handwritten digit database},
	author       = {LeCun, Yann and Cortes, Corinna},
	year         = 2010,
	journal      = {ATT Labs [Online]},
	url          = {http://yann.lecun.com/exdb/mnist/},
	added-at     = {2010-06-28T21:16:30.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	groups       = {public},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	interhash    = {21b9d0558bd66279df9452562df6e6f3},
	intrahash    = {935bad99fa1f65e03c25b315aa3c1032},
	keywords     = {MSc \_checked character\_recognition mnist network neural},
	lastchecked  = {2016-01-14 14:24:11},
	timestamp    = {2016-07-12T19:25:30.000+0200},
	username     = {mhwombat}
}
@inproceedings{reed2015deep,
	title        = {Deep Visual Analogy-Making},
	author       = {Scott Reed and Yi Zhang and Yuting Zhang and Honglak Lee},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@inproceedings{netzerReadingDigitsNatural,
	title        = {Reading Digits in Natural Images with Unsupervised Feature Learning},
	author       = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
	year         = 2011,
	booktitle    = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
	keywords     = {dataset}
}
@misc{3dshapes18,
	title        = {3Dshapes Dataset},
	author       = {Burgess, Chris and Kim, Hyunjik},
	year         = 2018,
	howpublished = {https://github.com/deepmind/3dshapes-dataset/}
}
@misc{dsprites17,
	title        = {dSprites: Disentanglement testing Sprites dataset},
	author       = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
	year         = 2017,
	howpublished = {https://github.com/deepmind/dsprites-dataset/}
}
@article{xiao2017/online,
	title        = {Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	year         = 2017,
	journal      = {arXiv:1708.07747},
	date         = {2017-08-28},
	eprintclass  = {cs.LG},
	eprinttype   = {arXiv},
	eprint       = {cs.LG/1708.07747}
}
@article{pmlr-v37-rezende15,
	title        = {Variational Inference with Normalizing Flows},
	author       = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	year         = 2015,
	journal      = {International Conference on Machine Learning (ICML)}
}
@inproceedings{Locatello2020Disentangling,
	title        = {Disentangling Factors of Variations Using Few Labels},
	author       = {Francesco Locatello and Michael Tschannen and Stefan Bauer and Gunnar R\"atsch and Bernhard Sch\"olkopf and Olivier Bachem},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations (ICLR)}
}
@inproceedings{locatelloChallengingassumptions,
	title        = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
	author       = {Francesco Locatello and Stefan Bauer and Mario Luvci\'c and Gunnar R\"atsch and Sylvain Gelly and Bernhard Sch\"olkopf and Olivier Frederic Bachem},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning (ICML)}
}
@inproceedings{isolatingsourcesdisentanglement,
	title        = {Isolating Sources of Disentanglement in VAEs},
	author       = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	location     = {Montr\'{e}al, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	pages        = {2615–2625},
	numpages     = 11
}
@inproceedings{ContractiveAE,
	title        = {Contractive Auto-Encoders: Explicit Invariance during Feature Extraction},
	author       = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
	year         = 2011,
	booktitle    = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	location     = {Bellevue, Washington, USA},
	publisher    = {Omnipress},
	address      = {Madison, WI, USA},
	series       = {ICML'11},
	pages        = {833–840},
	isbn         = 9781450306195,
	numpages     = 8
}
@inproceedings{bozkurt2018,
	title        = {Can VAEs Generate Novel Examples?},
	author       = {Alican Bozkurt and Babak Esmaeili and Dana H. Brooks and Jennifer G. Dy and Jan-Willem van de Meent},
	year         = 2018,
	booktitle    = {In Critiquing and Correcting Trends in Machine Learning Workshop at NeurIPS}
}
@inproceedings{kumar2018variational,
	title        = {Variational Inference of Disentangled Latent Concepts From Unlabeled Observations},
	author       = {Abhishek Kumar and Prasanna Sattigeri and Avinash Balakrishnan},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=H1kG7GZAW}
}
@inproceedings{zietlow21a,
	title        = {Demystifying Inductive Biases for (Beta-)VAE Based Architectures},
	author       = {Dominik Zietlow and Michal Rol\'inek and Georg Martius},
	year         = 2021,
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {12945--12954},
	url          = {http://proceedings.mlr.press/v139/zietlow21a.html},
	editor       = {Marina Meila and Tong Zhang},
	timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/ZietlowRM21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Robinek,
	title        = {Variational Autoencoders Pursue PCA Directions (by Accident)},
	author       = {Michal Rol\'inek and Dominik Zietlow and Georg Martius},
	year         = 2019,
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {12398--12407}
}
@inproceedings{Li2020Efficient,
	title        = {Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform},
	author       = {Jun Li and Fuxin Li and Sinisa Todorovic},
	year         = 2020,
	booktitle    = {proceedings of the International Conference on Learning Representations (ICLR)},
	abstract     = {Strictly enforcing orthonormality constraints on parameter matrices has been shown advantageous in deep learning. This amounts to Riemannian optimization on the Stiefel manifold, which, however, is...},
	keywords     = {optimization,stiefel}
}
@article{peyre2019computational,
	title        = {Computational optimal transport},
	author       = {Peyr\'e, Gabriel and Cuturi, Marco and others},
	year         = 2019,
	journal      = {Foundations and Trends\textregistered in Machine Learning},
	publisher    = {Now Publishers, Inc.},
	volume       = 11,
	number       = {5-6},
	pages        = {355--607}
}
@article{gilesExtendedCollectionMatrix,
	title        = {An Extended Collection of Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation},
	author       = {Giles, Mike},
	pages        = 23,
	language     = {en}
}
@inproceedings{houthuys_tensor-based_nodate,
	title        = {Tensor Learning in Multi-View Kernel  PCA},
	author       = {Houthuys, Lynn and Suykens, Johan A K},
	year         = 2018,
	booktitle    = {27th International Conference on Artificial Neural Networks {ICANN}, Rhodes, Greece},
	volume       = 11140,
	pages        = {205--215},
	language     = {en}
}
@inproceedings{joachim,
	title        = {Generative Kernel {{PCA}}},
	author       = {Schreurs, Joachim and Suykens, Johan A. K.},
	year         = 2018,
	booktitle    = {{{European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning }}},
	pages        = {129--134},
	abstract     = {Kernel PCA has shown to be a powerful feature extractor within many applications. Using the Restricted Kernel Machine formulation, a representation using visible and hidden units is obtained. This enables the exploration of new insights and connections between Restricted Boltzmann machines and kernel methods. This paper explores these connections, introducing a generative kernel PCA which can be used to generate new data, as well as denoise a given training dataset. This in a non-probabilistic setting. Moreover, relations with linear PCA and a preimage reconstruction method are introduced in this paper.},
	keywords     = {Kernel method,Kernel principal component analysis,Linux,List of Code Lyoko episodes,Noise reduction,Randomness extractor,Restricted Boltzmann machine,Smoothing}
}
@inproceedings{robust2020,
	title        = {Robust Generative Restricted Kernel Machines using Weighted Conjugate Feature Duality},
	author       = {Arun Pandey and Joachim Schreurs and Johan A. K. Suykens},
	year         = 2020,
	journal      = {Accepted at The Sixth International Conference on Machine Learning, Optimization, and Data Science, LOD},
	booktitle    = {proceedings of the Sixth International Conference on Machine Learning, Optimization, and Data Science (LOD)},
	location     = {Siena – Tuscany, Italy},
	venue        = {Siena – Tuscany, Italy}
}
@inproceedings{Heusel2017,
	title        = {GANs Trained by a Two Time-scale Update Rule Converge to a Local Nash Equilibrium},
	author       = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	location     = {Long Beach, California, USA},
	publisher    = {Curran Associates Inc.},
	address      = {USA},
	series       = {NIPS'17},
	pages        = {6629--6640},
	isbn         = {978-1-5108-6096-4},
	url          = {http://dl.acm.org/citation.cfm?id=3295222.3295408},
	numpages     = 12,
	acmid        = 3295408
}
@article{GENRKM,
	title        = {Generative Restricted Kernel Machines: A framework for multi-view generation and disentangled feature learning},
	author       = {Arun Pandey and Joachim Schreurs and Johan A.K. Suykens},
	year         = 2021,
	journal      = {Neural Networks},
	volume       = 135,
	pages        = {177--191}
}
@book{suykens_least_2002,
	title        = {Least Squares Support Vector Machines},
	author       = {Suykens, Johan A. K. and Van Gestel, Tony and De Brabanter, Jos and De Moor, Bart and Vandewalle, Joos},
	year         = 2002,
	month        = jan,
	publisher    = {World Scientific},
	address      = {River Edge, NJ},
	isbn         = {978-981-238-151-4},
	lccn         = {Q325.5 .L45 2002},
	language     = {en},
	keywords     = {Kernel functions,Least squares,Support vector machines}
}
@article{metz2016unrolled,
	title        = {Unrolled generative adversarial networks},
	author       = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
	year         = 2017,
	journal      = {ICLR}
}
@incollection{papadopouloEstimatingJacobianSingular2000a,
	title        = {Estimating the Jacobian of the Singular Value Decomposition: Theory and Applications},
	shorttitle   = {Estimating the {{Jacobian}} of the {{Singular Value Decomposition}}},
	author       = {Papadopoulo, Th\'eodore and Lourakis, Manolis I. A.},
	year         = 2000,
	booktitle    = {Computer {{Vision}} - {{ECCV}} 2000},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	volume       = 1842,
	pages        = {554--570},
	doi          = {10.1007/3-540-45054-8\_36},
	isbn         = {978-3-540-67685-0 978-3-540-45054-2},
	note         = {Series Title: Lecture Notes in Computer Science},
	language     = {en}
}
@inproceedings{salimans2016improved,
	title        = {Improved techniques for training gans},
	author       = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	year         = 2016,
	booktitle    = {Advances in neural information processing systems},
	pages        = {2234--2242}
}
@inproceedings{eastwood2018a,
	title        = {A Framework for the Quantitative Evaluation of Disentangled Representations},
	author       = {Cian Eastwood and Christopher K. I. Williams},
	year         = 2018,
	booktitle    = {proceedings of the International Conference on Learning Representations (ICLR)},
	url          = {https://openreview.net/forum?id=By-7dz-AZ}
}
@article{honeine_preimage_2011-1,
	title        = {Preimage Problem in Kernel-Based Machine Learning},
	author       = {Honeine, Paul and Richard, Cedric},
	year         = 2011,
	month        = mar,
	journal      = {IEEE Signal Processing Magazine},
	volume       = 28,
	number       = 2,
	pages        = {77--88},
	issn         = {1053-5888},
	abstract     = {While the nonlinear mapping from the input space to the feature space is central in kernel methods, the reverse mapping from the feature space back to the input space is also of primary interest. This is the case in many applications, including kernel principal component analysis (PCA) for signal and image denoising. Unfortunately, it turns out that the reverse mapping generally does not exist and only a few elements in the feature space have a valid preimage in the input space. The preimage problem consists of finding an approximate solution by identifying data in the input space based on their corresponding features in the high dimensional feature space. It is essentially a dimensionality-reduction problem, and both have been intimately connected in their historical evolution, as studied in this article.},
	keywords     = {Classification algorithms,dimensionality-reduction problem,Kernel,kernel methods,kernel-based machine learning,learning (artificial intelligence),Machine learning,Noise reduction,nonlinear mapping,Optimization,preimage problem,principal component analysis,Principal component analysis,reverse mapping,Signal processing algorithms}
}
@article{mercer_james_functions,
	title        = {Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations},
	author       = {Mercer, James},
	year         = 1909,
	month        = jan,
	journal      = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	volume       = 209,
	number       = {441-458},
	pages        = {415--446},
	abstract     = {The present memoir is the outcome of an attempt to obtain the conditions under which a given symmetric and continuous function k(s, t) is definite, in the sense of Hilbert. At an early stage, however, it was found that the class of definite functions was too restricted to allow the determination of necessary and sufficient conditions in terms of the determinants of \textsection{} 10. The discovery that this could be done for functions of positive or negative type, and the fact that almost all the theorems which are true of definite functions are, with slight modification, true of these, led finally to the abandonment of the original plan in favour of a discussion of the properties of functions belonging to the wider classes. The first part of the memoir is devoted to the definition of various terms employed, and to the re-statement of the consequences which follow from Hilbert's theorem.}
}
@inproceedings{salakhutdinov_deep,
	title        = {Deep Boltzmann Machines},
	author       = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
	year         = 2009,
	month        = apr,
	journal      = {Proceedings of the 12th International Conference on Artificial Intelligence and Statistics},
	booktitle    = {proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
	publisher    = {PMLR},
	volume       = {5 of JMLR},
	pages        = {448--455},
	issn         = {1938-7228},
	abstract     = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to fo...},
	language     = {en}
}
@inproceedings{hinton2005,
	title        = {What Kind of a Graphical Model is the Brain?},
	author       = {Hinton, Geoffrey E.},
	year         = 2005,
	booktitle    = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
	location     = {Edinburgh, Scotland},
	publisher    = {Morgan Kaufmann Publishers Inc.},
	address      = {San Francisco, CA, USA},
	series       = {IJCAI'05},
	pages        = {1765–1775},
	abstract     = {If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep, multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1:7 million connections into a very good generative model of handwritten digits. After learning, the model gives classification performance that is comparable to the best discriminative methods.},
	numpages     = 11
}
@article{bengio2013representation,
	title        = {Representation Learning: A Review and New Perspectives},
	author       = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	year         = 2013,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	publisher    = {IEEE},
	volume       = 35,
	number       = 8,
	pages        = {1798--1828}
}
@inproceedings{ghosh2019variational,
	title        = {From Variational to Deterministic Autoencoders},
	author       = {Ghosh, Partha and Sajjadi, Mehdi SM and Vergari, Antonio and Black, Michael and Sch\"olkopf, Bernhard},
	year         = 2020,
	journal      = {arXiv preprint arXiv:1903.12436},
	booktitle    = {proceedings of the International Conference on Learning Representations (ICLR)}
}
@inproceedings{karras2017progressive,
	title        = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
	author       = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year         = 2017,
	booktitle    = {proceedings of the International Conference on Learning Representations (ICLR)}
}
@inproceedings{higgins2017beta,
	title        = {Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
	author       = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year         = 2017,
	journal      = {International Conference on Learning Representations},
	booktitle    = {proceedings of the International Conference on Learning Representations (ICLR)},
	volume       = 2,
	number       = 5,
	pages        = 6,
	keywords     = {disentanglement,vae}
}
@inproceedings{burgess2018understanding,
	title        = {Understanding Disentangling in $\beta$-VAE},
	author       = {Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1804.03599},
	booktitle    = {NIPS 2017 Workshop on Learning Disentangled Representations: from Perception to Control}
}
@inproceedings{kingma_auto_encoding_2013,
	title        = {Auto-Encoding Variational Bayes},
	author       = {Diederik P. Kingma and Max Welling},
	year         = 2014,
	booktitle    = {proceedings of the International Conference on Learning Representations (ICLR)},
	timestamp    = {Thu, 04 Apr 2019 13:20:07 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{scholkopf1997kernel,
	title        = {Kernel principal component analysis},
	author       = {Sch\"olkopf, Bernhard and Smola, Alexander and M\"uller, Klaus-Robert},
	year         = 1997,
	booktitle    = {International conference on artificial neural networks},
	pages        = {583--588},
	organization = {Springer}
}
@inproceedings{lecun_learning_2004,
	title        = {Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting},
	author       = {LeCun, Yann and Huang, Fu Jie and Bottou, Leon},
	year         = 2004,
	booktitle    = {Computer Vision and Pattern Recognition (CVPR)},
	volume       = 2,
	pages        = {II-97--104 Vol.2},
	issn         = {1063-6919},
	abstract     = {We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13\% for SVM and 7\% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7\% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.},
	added-at     = {2009-09-12T19:19:34.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/25154220ccf84ad2961669b708f548dd3/mozaher},
	interhash    = {d2c62c571303e707eb0f13d01186c758},
	intrahash    = {5154220ccf84ad2961669b708f548dd3},
	keywords     = {analysis, component computer convolutional databases, detection, error extraction, feature generic grayscale image images, invariance, large learning lighting low-resolution machines, methods, nearest neighbor networks, object pose principal processing, rates recognition, segmentation, stereo support test vector very vision, visual},
	owner        = {mozaher},
	timestamp    = {2009-09-12T19:19:41.000+0200}
}
@book{Scholkopf2001,
	title        = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
	author       = {Sch\"olkopf , Bernhard and Smola, Alexander J.},
	year         = 2001,
	publisher    = {MIT Press},
	isbn         = {0262194759}
}
@article{dumoulin2016guide,
	title        = {A guide to convolution arithmetic for deep learning},
	author       = {Dumoulin, Vincent and Visin, Francesco},
	year         = 2016,
	journal      = {arXiv:1603.07285}
}
@inproceedings{dupont2018learning,
	title        = {Learning disentangled joint continuous and discrete representations},
	author       = {Dupont, Emilien},
	year         = 2018,
	booktitle    = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	pages        = {708--718}
}
@book{petersen_matrix_nodate,
	title        = {The Matrix Cookbook},
	author       = {K. B. Petersen and M. S. Pedersen},
	year         = 2012,
	month        = nov,
	publisher    = {Technical University of Denmark},
	note         = {Version 20121115},
	keywords     = {Matrix identity, matrix relations, inverse, matrix derivative},
	abstract     = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.}
}
@book{hastie01statisticallearning,
	title        = {The Elements of Statistical Learning},
	author       = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year         = 2001,
	publisher    = {Springer New York Inc.},
	address      = {New York, NY, USA},
	keywords     = {ml statistics}
}
@techreport{cifar_10,
	title        = {Learning multiple layers of features from tiny images},
	author       = {Alex Krizhevsky},
	year         = 2009,
	institution  = {{University} of {Toronto}}
}
@inproceedings{liu2015faceattributes,
	title        = {Deep Learning Face Attributes in the Wild},
	author       = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	year         = 2015,
	month        = dec,
	booktitle    = {Proceedings of International Conference on Computer Vision (ICCV)}
}
@article{Adam,
	title        = {Adam: A method for stochastic optimization},
	author       = {Kingma, Diederik P and Ba, Jimmy},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1412.6980},
	booktitle    = {The 3rd {{International Conference}} on {{Learning Representations}}}
}
@book{rockafeller1987,
	title        = {Conjugate Duality and Optimization},
	author       = {Rockafellar, Ralph Tyrrell},
	year         = 1974,
	publisher    = {SIAM}
}
@article{kwok_pre-image_2004-2,
	title        = {The pre-image problem in kernel methods},
	author       = {James T. Kwok and Ivor Wai-Hung Tsang},
	year         = 2003,
	journal      = {IEEE Transactions on Neural Networks},
	volume       = 15,
	pages        = {1517--1525}
}
@inproceedings{larochelle_classification,
	title        = {Classification Using Discriminative Restricted Boltzmann Machines},
	author       = {Larochelle, Hugo and Bengio, Yoshua},
	year         = 2008,
	booktitle    = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
	publisher    = {ACM Press},
	address      = {Helsinki, Finland},
	pages        = {536--543},
	isbn         = {978-1-60558-205-4},
	abstract     = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
	language     = {en}
}
@inproceedings{Yeh_2017_CVPR,
	title        = {Semantic Image Inpainting With Deep Generative Models},
	author       = {Yeh, Raymond A. and Chen, Chen and Yian Lim, Teck and Schwing, Alexander G. and Hasegawa-Johnson, Mark and Do, Minh N.},
	year         = 2017,
	month        = jul,
	booktitle    = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{florensa_automatic,
	title        = {Automatic Goal Generation for Reinforcement Learning Agents},
	author       = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
	year         = 2018,
	month        = {10--15 Jul},
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {Stockholmsm\"{a}ssan, Stockholm Sweden},
	series       = {Proceedings of Machine Learning Research},
	volume       = 80,
	pages        = {1515--1528},
	pdf          = {http://proceedings.mlr.press/v80/florensa18a/florensa18a.pdf},
	abstract     = {Reinforcement learning (RL) is a powerful technique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to accomplish, each task being specified as reaching a certain parametrized subset of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent, thus automatically producing a curriculum. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment, even when only sparse rewards are available. Videos and code available at https://sites.google.com/view/goalgeneration4rl.}
}
@article{vincent_stacked_nodate,
	title        = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
	author       = {Pascal Vincent and Hugo Larochelle and Isabelle Lajoie and Yoshua Bengio and Pierre-Antoine Manzagol},
	year         = 2010,
	journal      = {Journal of Machine Learning Research},
	volume       = 11,
	pages        = {3371--3408}
}
@article{wavenet,
	title        = {WaveNet: A Generative Model for Raw Audio},
	author       = {A\"aron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew W. Senior and Koray Kavukcuoglu},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1609.03499},
	url          = {http://arxiv.org/abs/1609.03499},
	archiveprefix = {arXiv},
	eprint       = {1609.03499},
	timestamp    = {Mon, 13 Aug 2018 16:49:15 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/OordDZSVGKSK16},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{weston_learning_2004,
 author = {Weston, Jason and Sch\"{o}lkopf, Bernhard and Bakir, G\"{o}khan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {},
 pages = {},
 publisher = {},
 title = {Learning to Find Pre-Images},
 url = {https://proceedings.neurips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf},
 volume = {16},
 year = {2003}
}


@incollection{pu_variational_2016,
	title        = {Variational Autoencoder for Deep Learning of Images, Labels and Captions},
	author       = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
	year         = 2016,
	location     = {Barcelona, Spain},
	publisher    = {Curran Associates Inc.},
	address      = {USA},
	series       = {NIPS'16},
	pages        = {2360--2368},
	isbn         = {978-1-5108-3881-9},
	numpages     = 9,
	acmid        = 3157360
}
@incollection{liu_coupled_2016,
	title        = {Coupled Generative Adversarial Networks},
	author       = {Liu, Ming-Yu and Tuzel, Oncel},
	year         = 2016,
	booktitle    = {Advances in {{Neural Information Processing Systems}} 29},
	publisher    = {Curran Associates, Inc.},
	pages        = {469--477}
}
@inproceedings{glorot_deep_2011,
	title        = {Deep Sparse Rectifier Neural Networks.},
	author       = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	year         = 2011,
	booktitle    = {International Conference on Artificial Intelligence and Statistics},
	publisher    = {JMLR.org},
	series       = {JMLR Proceedings},
	volume       = 15,
	pages        = {315--323},
	added-at     = {2014-04-01T20:16:10.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/256f5ffd25378f109c8cc14394bcfdabb/prlz77},
	editor       = {Gordon, Geoffrey J. and Dunson, David B. and Dud\'{\i}k, Miroslav},
	ee           = {http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf},
	interhash    = {fbf04ef5079b11118f3f3184b1068d88},
	intrahash    = {56f5ffd25378f109c8cc14394bcfdabb},
	keywords     = {Bengio Deep Networks Neural Rectifier Relu Sparse},
	timestamp    = {2014-04-01T20:16:10.000+0200},
	crossref     = {conf/aistats/2011}
}
@misc{behrmann2019invariance,
	title        = {Invariance and Inverse Stability under ReLU},
	author       = {Jens Behrmann and S\"oren Dittmer and Pascal Fernsel and Peter Maass},
	year         = 2019
}
@inproceedings{goodfellow_generative_2014,
	title        = {Generative Adversarial Nets},
	author       = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},
	year         = 2014,
	booktitle    = {Advances in Neural Information Processing Systems 27},
	pages        = {2672--2680},
	timestamp    = {Wed, 10 Dec 2014 21:34:12 +0100},
	biburl       = {https://dblp.org/rec/bib/conf/nips/GoodfellowPMXWOCB14},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{bui_projection-free_2019,
	title        = {Projection-Free Kernel Principal Component Analysis for Denoising},
	author       = {Bui, Anh Tuan and Im, Joon-Ku and Apley, Daniel W. and Runger, George C.},
	year         = 2019,
	journal      = {Neurocomputing},
	issn         = {0925-2312},
	abstract     = {Kernel principal component analysis (KPCA) forms the basis for a class of methods commonly used for denoising a set of multivariate observations. Most KPCA algorithms involve two steps: projection and preimage approximation. We argue that this two-step procedure can be inefficient and result in poor denoising. We propose an alternative projection-free KPCA denoising approach that does not involve the usual projection and subsequent preimage approximation steps. In order to denoise an observation, our approach performs a single line search along the gradient descent direction of the squared projection error. The rationale is that this moves an observation towards the underlying manifold that represents the noiseless data in the most direct manner possible. We demonstrate that the approach is simple, computationally efficient, robust, and sometimes provides substantially better denoising than the standard KPCA algorithm.},
	keywords     = {Feature space,Image processing,Pattern recognition,Preimage problem}
}
@article{billard_advanced_nodate,
	title        = {Advanced Machine Learning Practical 1 Solution: Manifold Learning (PCA and Kernel PCA)},
	author       = {Billard, Aude and Figueroa, Nadia and Lauzana, Ilaria and Platerrier, Brice},
	pages        = 24,
	language     = {en}
}
@inproceedings{mika_kernel_nodate,
	title        = {Kernel PCA and De-noising in Feature Spaces},
	author       = {Mika, Sebastian and Sch\"olkopf, Bernhard and Smola, Alex and M\"uller, Klaus-Robert and Scholz, Matthias and R\"atsch, Gunnar},
	year         = 1999,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {MIT Press},
	pages        = {536--542},
	numpages     = 7,
	acmid        = 340729
}
@incollection{Smolensky:1986,
	title        = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
	author       = {Smolensky, Paul},
	year         = 1986,
	publisher    = {MIT Press},
	address      = {Cambridge, MA, USA},
	pages        = {194--281},
	isbn         = {0-262-68053-X},
	chapter      = {Information Processing in Dynamical Systems: Foundations of Harmony Theory},
	editor       = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
	numpages     = 88,
	acmid        = 104290
}
@inproceedings{salakhutdinov_restricted,
	title        = {Restricted Boltzmann Machines for Collaborative Filtering},
	author       = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
	year         = 2007,
	booktitle    = {{{ICML}} '07},
	publisher    = {ACM Press},
	address      = {Corvalis, Oregon},
	pages        = {791--798},
	abstract     = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6\% better than the score of Netflix's own system.},
	language     = {en}
}
@inproceedings{oord_pixel_2016,
	title        = {Pixel Recurrent Neural Networks},
	author       = {Van Den Oord, A\"aron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	year         = 2016,
	booktitle    = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	location     = {New York, NY, USA},
	publisher    = {JMLR.org},
	series       = {ICML'16},
	pages        = {1747--1756},
	numpages     = 10,
	acmid        = 3045575
}
@article{schmidhuber1992learning,
	title        = {Learning factorial codes by predictability minimization},
	author       = {Schmidhuber, J\"urgen},
	year         = 1992,
	journal      = {Neural Computation},
	publisher    = {MIT Press},
	volume       = 4,
	number       = 6,
	pages        = {863--879}
}
@article{ridgeway2016survey,
	title        = {A Survey of Inductive Biases for Factorial Representation-Learning},
	author       = {Karl Ridgeway},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1612.05299},
	eprint       = {1612.05299}
}
@article{Lawrence:2005:PNP:1046920.1194904,
	title        = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
	author       = {Lawrence, Neil},
	year         = 2005,
	month        = dec,
	journal      = {JMLR},
	publisher    = {JMLR.org},
	volume       = 6,
	pages        = {1783--1816},
	issn         = {1532-4435},
	url          = {http://dl.acm.org/citation.cfm?id=1046920.1194904},
	issue_date   = {12/1/2005},
	numpages     = 34,
	acmid        = 1194904
}
@article{Rabiner86anintroduction,
	title        = {An introduction to Hidden Markov models},
	author       = {Rabiner, Lawrence R and Juang, Biing-Hwang},
	year         = 1986,
	journal      = {IEEE ASSP magazine},
	publisher    = {Citeseer},
	volume       = 3,
	number       = 1,
	pages        = {4--16}
}
@article{Tipping99probabilisticprincipal,
	title        = {Probabilistic Principal Component Analysis},
	author       = {Michael E. Tipping and Chris M. Bishop},
	year         = 1999,
	journal      = {Journal Of The Royal Statistical Society, series B},
	volume       = 61,
	number       = 3,
	pages        = {611--622}
}
@inproceedings{alemi2016deep,
	title        = {Deep Variational Information Bottleneck},
	author       = {Alex Alemi and Ian Fischer and Josh Dillon and Kevin Murphy},
	year         = 2017,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{bouchacourt2018multi,
	title        = {Multi-level variational autoencoder: Learning disentangled representations from grouped observations},
	author       = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
	year         = 2018,
	booktitle    = {Thirty-Second AAAI Conference on Artificial Intelligence}
}
@inproceedings{tran2017disentangled,
	title        = {Disentangled representation learning GAN for pose-invariant face recognition},
	author       = {Tran, Luan and Yin, Xi and Liu, Xiaoming},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {1415--1424}
}
@inproceedings{chen2016infogan,
	title        = {Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
	author       = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {2172--2180}
}
@inproceedings{chen2017multi,
	title        = {Multi-view generative adversarial networks},
	author       = {Chen, Micka\"el and Denoyer, Ludovic},
	year         = 2017,
	booktitle    = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages        = {175--188},
	organization = {Springer}
}
@inproceedings{krizhevsky2012imagenet,
	title        = {Imagenet classification with deep convolutional neural networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {1097--1105}
}
@article{kramer1991nonlinear,
	title        = {Nonlinear principal component analysis using autoassociative neural networks},
	author       = {Kramer, Mark A},
	year         = 1991,
	journal      = {AIChE journal},
	publisher    = {Wiley Online Library},
	volume       = 37,
	number       = 2,
	pages        = {233--243}
}
@article{suykens2003support,
	title        = {A support vector machine formulation to PCA analysis and its kernel version},
	author       = {Suykens, Johan \relax A.K and Van Gestel, Tony and Vandewalle, Joos and De Moor, Bart},
	year         = 2003,
	journal      = {IEEE Transactions on Neural Networks},
	publisher    = {IEEE},
	volume       = 14,
	number       = 2,
	pages        = {447--450}
}
@article{alzate_multiway_2010,
	title        = {Multiway Spectral Clustering with Out-of-Sample Extensions through Weighted Kernel PCA},
	author       = {Alzate, C. and Suykens, J.A.K.},
	year         = 2010,
	month        = feb,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	publisher    = {IEEE},
	volume       = 32,
	number       = 2,
	pages        = {335--347},
	doi          = {10.1109/TPAMI.2008.292},
	issn         = {0162-8828},
	abstract     = {A new formulation for multiway spectral clustering is proposed. This method corresponds to a weighted kernel principal component analysis (PCA) approach based on primal-dual least-squares support vector machine (LS-SVM) formulations. The formulation allows the extension to out-of-sample points. In this way, the proposed clustering model can be trained, validated, and tested. The clustering information is contained on the eigendecomposition of a modified similarity matrix derived from the data. This eigenvalue problem corresponds to the dual solution of a primal optimization problem formulated in a high-dimensional feature space. A model selection criterion called the Balanced Line Fit (BLF) is also proposed. This criterion is based on the out-of-sample extension and exploits the structure of the eigenvectors and the corresponding projections when the clusters are well formed. The BLF criterion can be used to obtain clustering parameters in a learning framework. Experimental results with difficult toy problems and image segmentation show improved performance in terms of generalization to new samples and computation times.},
	language     = {en}
}
@phdthesis{belkin_problems_nodate,
	title        = {Problems of Learning on Manifolds (PhD Thesis)},
	author       = {Belkin, Mikahil},
	note         = {Ph.D. Thesis, UChicago}
}
@article{w._arthur_c._1979,
	title        = {C. T. H. Baker, The Numerical Treatment of Integral Equations},
	author       = {W. Arthur, D},
	year         = 1979,
	month        = feb,
	journal      = {Proceedings of The Edinburgh Mathematical Society - PROC EDINBURGH MATH SOC},
	volume       = 22,
	doi          = {10.1017/S0013091500027863}
}
@article{hubert_comparing_1985,
	title        = {Comparing Partitions},
	author       = {Hubert, Lawrence J. and Arabie, Phipps},
	year         = 1985,
	journal      = {Journal of Classification},
	volume       = 2,
	number       = {2-3},
	pages        = {193--218},
	doi          = {10.1007/BF01908075},
	issn         = {0176-4268(Print)},
	abstract     = {Reviews a measure of partition correspondence, the Rand index, and discusses how it might be normalized to provide a more appropriate descriptive measure. The general problem of comparing partitions is approached indirectly by assessing the congruence of 2 proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. A measure is proposed based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {$\pm$}1. (34 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	keywords     = {Statistical Norms,Statistical Tests}
}
@book{Boyd:2004:CO:993483,
	title        = {Convex Optimization},
	author       = {Boyd, Stephen and Vandenberghe, Lieven},
	year         = 2004,
	publisher    = {Cambridge University Press},
	address      = {New York, NY, USA},
	isbn         = {0521833787}
}
@inproceedings{arthur_k-means++_2007,
	title        = {K-Means++: The Advantages of Careful Seeding},
	shorttitle   = {K-Means++},
	author       = {Arthur, David and Vassilvitskii, Sergei},
	year         = 2007,
	booktitle    = {In {{Proceedings}} of the 18th {{Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
	abstract     = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is {$\Theta$}(log k)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically. 1}
}
@phdthesis{lafon_diffusion_nodate,
	title        = {Diffusion Maps and Geometric Harmonics (PhD Dissertion)},
	author       = {Lafon, St\'ephane S},
	language     = {en}
}
@article{canzani_analysis_nodate,
	title        = {Analysis on Manifolds via the Laplacian},
	author       = {Canzani, Yaiza},
	pages        = 114,
	language     = {en}
}
@article{houthuys2018multi,
	title        = {Multi-view kernel spectral clustering},
	author       = {Houthuys, Lynn and Langone, Rocco and Suykens, Johan \relax A.K},
	year         = 2018,
	journal      = {Information Fusion},
	publisher    = {Elsevier},
	volume       = 44,
	pages        = {46--56}
}
@article{suykens2002weighted,
	title        = {Weighted least squares support vector machines: robustness and sparse approximation},
	author       = {Suykens, Johan  A.K. and De Brabanter, Jos and Lukas, Lukas and Vandewalle, Joos},
	year         = 2002,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 48,
	number       = {1-4},
	pages        = {85--105}
}
@book{rousseeuw2005robust,
	title        = {Robust regression and outlier detection},
	author       = {Rousseeuw, Peter J and Leroy, Annick M},
	year         = 2005,
	publisher    = {John wiley \& sons},
	volume       = 589
}
@article{mathieu2018disentangling,
	title        = {Disentangling Disentanglement},
	author       = {Mathieu, Emile and Rainforth, Tom and Narayanaswamy, Siddharth and Teh, Yee Whye},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1812.02833}
}
@inproceedings{alzate2006weighted,
	title        = {A weighted kernel PCA formulation with out-of-sample extensions for spectral clustering methods},
	author       = {Alzate, Carlos and Suykens, Johan \relax A.K},
	year         = 2006,
	booktitle    = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
	pages        = {138--144},
	organization = {IEEE}
}
@article{rousseeuw1999fast,
	title        = {A fast algorithm for the minimum covariance determinant estimator},
	author       = {Rousseeuw, Peter J and Driessen, Katrien Van},
	year         = 1999,
	journal      = {Technometrics},
	publisher    = {Taylor \& Francis Group},
	volume       = 41,
	number       = 3,
	pages        = {212--223}
}
@article{croux1999influence,
	title        = {Influence function and efficiency of the minimum covariance determinant scatter matrix estimator},
	author       = {Croux, Christophe and Haesbroeck, Gentiane},
	year         = 1999,
	journal      = {Journal of Multivariate Analysis},
	publisher    = {Elsevier},
	volume       = 71,
	number       = 2,
	pages        = {161--190}
}
@article{hubert2012deterministic,
	title        = {A deterministic algorithm for robust location and scatter},
	author       = {Hubert, Mia and Rousseeuw, Peter J and Verdonck, Tim},
	year         = 2012,
	journal      = {Journal of Computational and Graphical Statistics},
	publisher    = {Taylor \& Francis},
	volume       = 21,
	number       = 3,
	pages        = {618--637}
}
@article{hubert2005robpca,
	title        = {ROBPCA: a new approach to robust principal component analysis},
	author       = {Hubert, Mia and Rousseeuw, Peter J and Vanden Branden, Karlien},
	year         = 2005,
	journal      = {Technometrics},
	publisher    = {Taylor \& Francis},
	volume       = 47,
	number       = 1,
	pages        = {64--79}
}
@article{rousseeuw1985multivariate,
	title        = {Multivariate estimation with high breakdown point},
	author       = {Rousseeuw, Peter J},
	year         = 1985,
	journal      = {Mathematical statistics and applications},
	volume       = 8,
	number       = {283-297},
	pages        = 37
}
@article{hubert2015dets,
	title        = {The DetS and DetMM estimators for multivariate location and scatter},
	author       = {Hubert, Mia and Rousseeuw, Peter and Vanpaemel, Dina and Verdonck, Tim},
	year         = 2015,
	journal      = {Computational Statistics \& Data Analysis},
	publisher    = {Elsevier},
	volume       = 81,
	pages        = {64--75}
}
@incollection{engelen2004robust,
	title        = {Robust PCR and Robust PLSR: a comparative study},
	author       = {Engelen, S and Hubert, M and Branden, K Vanden and Verboven, S},
	year         = 2004,
	booktitle    = {Theory and Applications of Recent Robust Methods},
	publisher    = {Springer},
	pages        = {105--117}
}
@article{hubert2004fastda,
	title        = {Fast and robust discriminant analysis},
	author       = {Hubert, Mia and Van Driessen, Katrien},
	year         = 2004,
	journal      = {Computational Statistics \& Data Analysis},
	publisher    = {Elsevier},
	volume       = 45,
	number       = 2,
	pages        = {301--320}
}
@inproceedings{goodfellow2014generative,
	title        = {Generative adversarial nets},
	author       = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year         = 2014,
	booktitle    = {Advances in NIPS},
	pages        = {2672--2680},
	keywords     = {gan}
}
@inproceedings{arjovsky2017wasserstein,
	title        = {Wasserstein generative adversarial networks},
	author       = {Arjovsky, Martin and Chintala, Soumith and Bottou, L\'eon},
	year         = 2017,
	booktitle    = {ICML},
	pages        = {214--223}
}
@article{cohen2014transformation,
	title        = {Transformation properties of learned visual representations},
	author       = {Cohen, Taco S and Welling, Max},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1412.7659}
}
@article{futami2017variational,
	title        = {Variational inference based on robust divergences},
	author       = {Futami, Futoshi and Sato, Issei and Sugiyama, Masashi},
	year         = 2018,
	journal      = {21st International Conference on Artificial Intelligence and Statistics}
}
@inproceedings{qi2014robust,
	title        = {Robust feature learning by stacked autoencoder with maximum correntropy criterion},
	author       = {Qi, Yu and Wang, Yueming and Zheng, Xiaoxiang and Wu, Zhaohui},
	year         = 2014,
	booktitle    = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {6716--6720},
	organization = {IEEE}
}
@inproceedings{vincent2008extracting,
	title        = {Extracting and composing robust features with denoising autoencoders},
	author       = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year         = 2008,
	booktitle    = {25th ICML},
	pages        = {1096--1103}
}
@inproceedings{kaneko2019label,
	title        = {Label-noise robust generative adversarial networks},
	author       = {Kaneko, Takuhiro and Ushiku, Yoshitaka and Harada, Tatsuya},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {2467--2476}
}
@article{chrysos2018robust,
	title        = {Robust conditional generative adversarial networks},
	author       = {Chrysos, Grigorios G and Kossaifi, Jean and Zafeiriou, Stefanos},
	year         = 2019,
	journal      = {International Conference on Learning Representations}
}
@article{alzate2008kernel,
	title        = {Kernel component analysis using an epsilon-insensitive robust loss function},
	author       = {Alzate, Carlos and Suykens, Johan \relax A.K},
	year         = 2008,
	journal      = {IEEE Transactions on Neural Networks},
	publisher    = {IEEE},
	volume       = 19,
	number       = 9,
	pages        = {1583--1598}
}
@inproceedings{nguyen2009robust,
	title        = {Robust kernel principal component analysis},
	author       = {Nguyen, Minh H and Torre, Fernando},
	year         = 2009,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {1185--1192}
}
@article{ren2018learning,
	title        = {Learning to reweight examples for robust deep learning},
	author       = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
	year         = 2018,
	journal      = {International conference on Machine learning (ICML)}
}
@article{azadi2015auxiliary,
	title        = {Auxiliary image regularization for deep cnns with noisy labels},
	author       = {Azadi, Samaneh and Feng, Jiashi and Jegelka, Stefanie and Darrell, Trevor},
	year         = 2016,
	journal      = {Proceedings of the 4th International Conference on Learning Representation}
}
@inproceedings{zenati2018adversarially,
	title        = {Adversarially learned anomaly detection},
	author       = {Zenati, Houssam and Romain, Manon and Foo, Chuan-Sheng and Lecouat, Bruno and Chandrasekhar, Vijay},
	year         = 2018,
	booktitle    = {2018 IEEE International Conference on Data Mining (ICDM)},
	pages        = {727--736},
	organization = {IEEE}
}
@inproceedings{tang2012robust,
	title        = {Robust boltzmann machines for recognition and denoising},
	author       = {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
	year         = 2012,
	booktitle    = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {2264--2271},
	organization = {IEEE}
}
@article{chen2020,
	title        = {Informative Outlier Matters: Robustifying Out-of-Distribution Detection Using Outlier Mining},
	shorttitle   = {Informative {{Outlier Matters}}},
	author       = {Chen, Jiefeng and Li, Yixuan and Wu, Xi and Liang, Yingyu and Jha, Somesh},
	year         = 2020,
	journal      = {arXiv:2006.15207 [cs, stat]},
	abstract     = {Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in an open-world setting. However, existing OOD detection solutions can be brittle in the open world, facing various types of adversarial OOD inputs. While methods leveraging auxiliary OOD data have emerged, our analysis reveals a key insight that the majority of auxiliary OOD examples may not meaningfully improve the decision boundary of the OOD detector. In this paper, we provide a theoretically motivated method, Adversarial Training with informative Outlier Mining (ATOM), which improves the robustness of OOD detection. We show that, by mining informative auxiliary OOD data, one can significantly improve OOD detection performance, and somewhat surprisingly, generalize to unseen adversarial attacks. ATOM achieves state-of-the-art performance under a broad family of natural and perturbed OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset, ATOM reduces the FPR95 by up to 57.99\% under adversarial OOD inputs, surpassing the previous best baseline by a large margin.},
	archiveprefix = {arXiv},
	eprint       = {2006.15207},
	eprinttype   = {arxiv},
	primaryclass = {cs, stat}
}
@article{choi2019,
	title        = {WAIC, but Why? Generative Ensembles for Robust Anomaly Detection},
	shorttitle   = {{{WAIC}}, but {{Why}}?},
	author       = {Choi, Hyunsun and Jang, Eric and Alemi, Alexander A.},
	year         = 2019,
	month        = may,
	journal      = {arXiv:1810.01392 [cs, stat]},
	abstract     = {Machine learning models encounter Out-of-Distribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation -- although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.},
	archiveprefix = {arXiv},
	eprint       = {1810.01392},
	eprinttype   = {arxiv},
	keywords     = {read later},
	primaryclass = {cs, stat}
}
@inproceedings{goodfellow2015,
	title        = {Explaining and Harnessing Adversarial Examples},
	author       = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year         = 2015,
	booktitle    = {The 3rd {{International Conference}} on {{Learning Representations}}}
}
@inproceedings{grathwohl2020,
	title        = {Your Classifier Is Secretly an Energy Based Model and You Should Treat It like One},
	author       = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Joern-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
	year         = 2020,
	booktitle    = {The 8th {{International Conference}} on {{Learning Representations}}}
}
@inproceedings{hendrycks2017,
	title        = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
	author       = {Hendrycks, Dan and Gimpel, Kevin},
	year         = 2017,
	booktitle    = {The 5th {{International Conference}} on {{Learning Representations}}},
	abstract     = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
	archiveprefix = {arXiv},
	eprint       = {1610.02136},
	eprinttype   = {arxiv}
}
@inproceedings{hendrycks2019,
	title        = {Deep Anomaly Detection with Outlier Exposure},
	author       = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
	year         = 2019,
	booktitle    = {The 7th {{International Conference}} on {{Learning Representations}}}
}
@inproceedings{lee2018,
	title        = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
	author       = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
	year         = 2018,
	booktitle    = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}}
}
@inproceedings{liang2018,
	title        = {Enhancing The Reliability of Out-of-Distribution Image Detection in Neural Networks},
	author       = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
	year         = 2018,
	booktitle    = {The 6th {{International Conference}} on {{Learning Representations}}}
}
@inproceedings{liu2018,
	title        = {Open Category Detection with PAC Guarantees},
	author       = {Liu, Si and Garrepalli, Risheek and Dietterich, Thomas and Fern, Alan and Hendrycks, Dan},
	year         = 2018,
	booktitle    = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
	publisher    = {PMLR},
	volume       = 80,
	pages        = {3169--3178},
	keywords     = {read later}
}
@inproceedings{liu2020a,
	title        = {Energy-Based Out-of-Distribution Detection},
	author       = {Liu, Weitang and Wang, Xiaoyun and Owens, John D. and Li, Yixuan},
	year         = 2020,
	booktitle    = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
	abstract     = {Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data. We propose a unified framework for OOD detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95\%) by 18.03\% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks.},
	archiveprefix = {arXiv},
	eprint       = {2010.03759},
	eprinttype   = {arxiv}
}
@inproceedings{nalisnick2019,
	title        = {Do Deep Generative Models Know What They Don't Know?},
	author       = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
	year         = 2019,
	booktitle    = {The 7th {{International Conference}} on {{Learning Representations}}}
}
@inproceedings{skvara2018,
	title        = {Are Generative Deep Models for Novelty Detection Truly Better?},
	author       = {\v Skv\'ara, V\'it and Pevn\'y, Tom\'a\v s and \v Sm\'idl, V\'aclav},
	year         = 2018,
	booktitle    = {{{The 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}}},
	abstract     = {Many deep models have been recently proposed for anomaly detection. This paper presents comparison of selected generative deep models and classical anomaly detection methods on an extensive number of non--image benchmark datasets. We provide statistical comparison of the selected models, in many configurations, architectures and hyperparamaters. We arrive to conclusion that performance of the generative models is determined by the process of selection of their hyperparameters. Specifically, performance of the deep generative models deteriorates with decreasing amount of anomalous samples used in hyperparameter selection. In practical scenarios of anomaly detection, none of the deep generative models systematically outperforms the kNN.},
	archiveprefix = {arXiv},
	eprint       = {1807.05027},
	eprinttype   = {arxiv}
}
@incollection{lecun2006,
	title        = {A Tutorial on Energy-Based Learning},
	author       = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M. and Huang, F.},
	year         = 2006,
	booktitle    = {Predicting Structured Data},
	publisher    = {MIT Press},
	pages        = {191--246},
	editor       = {BakIr, G{\"o}khan and Hofmann, Thomas and Sch{\"o}lkopf, Bernhard and Smola, Alexander J. and Taskar, Ben and Vishwanathan, S.V. N.},
	keywords     = {energy}
}
@article{fashionmnist,
	title        = {Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	shorttitle   = {Fashion-{{MNIST}}},
	author       = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	year         = 2017,
	journal      = {arXiv:1708.07747 [cs, stat]},
	abstract     = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	archiveprefix = {arXiv},
	eprint       = {1708.07747},
	eprinttype   = {arxiv},
	keywords     = {dataset},
	primaryclass = {cs, stat}
}
@misc{cifar10,
	title        = {Learning Multiple Layers of Features from Tiny Images},
	author       = {Krizhevsky, Alex},
	year         = 2009,
	howpublished = {\url{https://www.cs.toronto.edu/~kriz/cifar.html}}
}
@misc{strkm,
	title        = {Disentangled Representation Learning and Generation with Manifold Optimization},
	author       = {Pandey, Arun and Fanuel, Michael and Schreurs, Joachim and Suykens, Johan A. K.},
	year         = 2020,
	journal      = {arXiv:2006.07046 [cs, stat]},
	url = {https://arxiv.org/abs/2006.07046},
	doi = {10.48550/ARXIV.2006.07046},
	abstract     = {Disentanglement is an enjoyable property in representation learning which increases the interpretability of generative models such as Variational Auto-Encoders (VAE), Generative Adversarial Models and their many variants. In the context of latent space models, this work presents a representation learning framework that explicitly promotes disentanglement thanks to the combination of an auto-encoder with Principal Component Analysis (PCA) in latent space. The proposed objective is the sum of an auto-encoder error term along with a PCA reconstruction error in the feature space. This has an interpretation of a Restricted Kernel Machine with an interconnection matrix on the Stiefel manifold. The construction encourages a matching between the principal directions in latent space and the directions of orthogonal variation in data space. The training algorithm involves a stochastic optimization method on the Stiefel manifold, which increases only marginally the computing time compared to an analogous VAE. Our theoretical discussion and various experiments show that the proposed model improves over many VAE variants along with special emphasis on disentanglement learning.},
	archiveprefix = {arXiv},
	eprint       = {2006.07046},
	eprinttype   = {arxiv},
	primaryclass = {cs, stat}
}
@article{inman1989,
	title        = {The Overlapping Coefficient as a Measure of Agreement between Probability Distributions and Point Estimation of the Overlap of Two Normal Densities},
	author       = {Inman, Henry F. and Jr, Edwin L. Bradley},
	year         = 1989,
	journal      = {Communications in Statistics - Theory and Methods},
	publisher    = {Taylor \& Francis},
	volume       = 18,
	number       = 10,
	pages        = {3851--3874},
	doi          = {10.1080/03610928908830127},
	issn         = {0361-0926},
	abstract     = {The overlapping coefficient is defined as a measure of the agreement between two probability distributions. Its relationship to the dissimilarity index and its propertie are described. An extensive treatment of maximum-likelihood estimation of the overlap between two normal distributions is presented as an example of estimating the overlapping coefficient from sample data.},
	annotation   = {\_eprint: https://doi.org/10.1080/03610928908830127}
}
@article{pastore2019,
	title        = {Measuring Distribution Similarities Between Samples: A Distribution-Free Overlapping Index},
	shorttitle   = {Measuring {{Distribution Similarities Between Samples}}},
	author       = {Pastore, Massimiliano and Calcagn\`i, Antonio},
	year         = 2019,
	journal      = {Frontiers in Psychology},
	publisher    = {Frontiers},
	volume       = 10,
	doi          = {10.3389/fpsyg.2019.01089},
	issn         = {1664-1078},
	abstract     = {Every day cognitive and experimental researchers attempt to find evidence in support of their hypotheses in terms of statistical differences or similarities among groups. The most typical cases involve quantifying the difference of two samples in terms of their mean values using the \$t\$ statistic or other measures, such as Cohen's \$d\$ or \$U\$ metrics. In both cases the aim is to quantify how large such differences have to be in order to be classified as notable effects. These issues are particularly relevant when dealing with experimental and applied psychological research. However, most of these standard measures require some distributional assumptions to be correctly used, such as symmetry, unimodality, and well-established parametric forms. Although these assumptions guarantee that asymptotic properties for inference are satisfied, they can often limit the validity and interpretability of results. In this article we illustrate the use of a distribution-free overlapping measure as an alternative way to quantify sample differences and assess research hypotheses expressed in terms of Bayesian evidence. Main features and potentials of the overlapping index are illustrated by means of three empirical applications. Results suggest that using this index can considerably improve the interpretability of data analysis results in psychological research, as well as the reliability of conclusions that researchers can draw from their studies.},
	language     = {English}
}
@inproceedings{fabius2015,
	title        = {Variational Recurrent Auto-Encoders},
	author       = {Fabius, Otto and van Amersfoort, Joost R. and Kingma, Diederik P.},
	year         = 2015,
	booktitle    = {Workshop at {{The}} 3rd {{International Conference}} on {{Learning Representations}}}
}
@misc{ecg5000,
	title        = {The UCR Time Series Classification Archive},
	author       = {Chen, Yanping and Keogh, Eamonn and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},
	year         = 2015,
	howpublished = {\url{https://www.cs.ucr.edu/~eamonn/time\_series\_data/}}
}
@book{pca,
	title        = {Principal Components Analysis},
	author       = {Jolliffe, Ian T.},
	year         = 1986,
	publisher    = {Springer}
}
@article{pimentel2014,
	title        = {A Review of Novelty Detection},
	author       = {Pimentel, Marco A. F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
	year         = 2014,
	journal      = {Signal Processing},
	volume       = 99,
	pages        = {215--249},
	doi          = {10.1016/j.sigpro.2013.12.026},
	issn         = {0165-1684},
	keywords     = {read later},
	language     = {en}
}
@article{rousseeuw2011,
	title        = {Robust Statistics for Outlier Detection},
	author       = {Rousseeuw, Peter J. and Hubert, Mia},
	year         = 2011,
	journal      = {WIREs Data Mining and Knowledge Discovery},
	volume       = 1,
	pages        = {73--79},
	doi          = {10.1002/widm.2},
	issn         = {1942-4795},
	annotation   = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.2},
	language     = {en}
}
@article{hodge2004,
	title        = {A Survey of Outlier Detection Methodologies},
	author       = {Hodge, Victoria J. and Austin, Jim},
	year         = 2004,
	journal      = {Artificial Intelligence Review},
	volume       = 22,
	number       = 2,
	pages        = {85--126},
	doi          = {10.1007/s10462-004-4304-y},
	issn         = {1573-7462},
	language     = {en}
}
@inproceedings{robustrkm,
	title        = {Robust Generative Restricted Kernel Machines Using Weighted Conjugate Feature Duality},
	author       = {Pandey, Arun and Schreurs, Joachim and Suykens, Johan A. K.},
	year         = 2020,
	booktitle    = {International {{Conference}} on {{Machine Learning}}, {{Optimization}}, and {{Data Science}}},
	abstract     = {In the past decade, interest in generative models has grown tremendously. However, their training performance can be highly affected by contamination, where outliers are encoded in the representation of the model. This results in the generation of noisy data. In this paper, we introduce a weighted conjugate feature duality in the framework of Restricted Kernel Machines (RKMs). This formulation is used to fine-tune the latent space of generative RKMs using a weighting function based on the Minimum Covariance Determinant, which is a highly robust estimator of multivariate location and scatter. Experiments show that the weighted RKM is capable of generating clean images when contamination is present in the training data. We further show that the robust method also preserves uncorrelated feature learning through qualitative and quantitative experiments on standard datasets.}
}
@inproceedings{vae,
	title        = {Auto-Encoding Variational Bayes},
	author       = {Kingma, Diederik P and Welling, Max},
	year         = 2014,
	booktitle    = {The 2nd {{International Conference}} on {{Learning Representations}}},
	keywords     = {vae}
}
@inproceedings{mohseni2020,
	title        = {Self-Supervised Learning for Generalizable out-of-Distribution Detection},
	author       = {Mohseni, Sina and Pitale, Mandar and Yadawa, J. B. S. and Wang, Zhangyang},
	year         = 2020,
	booktitle    = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
	volume       = 34,
	pages        = {5216--5223}
}
@article{sketchy2016,
	title        = {The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies},
	author       = {Patsorn Sangkloy and Nathan Burnell and Cusuh Ham and James Hays},
	year         = 2016,
	journal      = {ACM Transactions on Graphics (proceedings of SIGGRAPH)}
}
@article{McInnes2018,
	title        = {UMAP: Uniform Manifold Approximation and Projection},
	author       = {Leland McInnes and John Healy and Nathaniel Saul and Lukas Grossberger},
	year         = 2018,
	journal      = {Journal of Open Source Software},
	publisher    = {The Open Journal},
	volume       = 3,
	number       = 29,
	pages        = 861,
	doi          = {10.21105/joss.00861}
}
@article{sangkloy2016sketchy,
	title        = {The sketchy database: learning to retrieve badly drawn bunnies},
	author       = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
	year         = 2016,
	journal      = {ACM Transactions on Graphics (TOG)},
	publisher    = {ACM New York, NY, USA},
	volume       = 35,
	number       = 4,
	pages        = {1--12}
}
@inproceedings{wu2018multimodal,
	title        = {Multimodal generative models for scalable weakly-supervised learning},
	author       = {Wu, Mike and Goodman, Noah},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {5575--5585}
}
@inproceedings{srivastava2012multimodal,
	title        = {Multimodal learning with deep boltzmann machines},
	author       = {Srivastava, Nitish and Salakhutdinov, Russ R},
	year         = 2012,
	booktitle    = {Advances in neural information processing systems},
	pages        = {2222--2230}
}
@article{suzuki2016joint,
	title        = {Joint multimodal learning with deep generative models},
	author       = {Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1611.01891}
}
@book{bishop_2006,
	title        = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	author       = {Bishop, Christopher M.},
	year         = 2006,
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	isbn         = {0387310738}
}
@inproceedings{FrRo2010Diffusion,
	title        = {A drift-filtered approach to diffusion estimation for multiscale processes},
	author       = {Frederix, Yves and Roose, Dirk},
	year         = 2010,
	booktitle    = {Coping with complexity: model reduction and data analysis},
	publisher    = {Springer-Verlag},
	series       = {Lecture Notes in Computational Science and Engineering},
	volume       = 75
}
@phdthesis{Meert2011PhD,
	title        = {Inference and Learning for Directed Probabilistic Logic Models},
	author       = {Meert, Wannes},
	year         = 2011,
	month        = Mar,
	pages        = {188 + xix},
	url          = {https://lirias.kuleuven.be/handle/123456789/298694},
	note         = {Blockeel, Hendrik (supervisor)},
	school       = {Informatics Section, Department of Computer Science, Faculty of Engineering}
}
@inproceedings{VandenBroeck2011IJCAI,
	title        = {Lifted probabilistic inference by first-order knowledge compilation},
	author       = {Van den Broeck, Guy and Taghipour, Nima and Meert, Wannes and Davis, Jesse and De Raedt, Luc},
	year         = 2011,
	booktitle    = {Proceedings of the 22th International Joint Conference on Artificial Intelligence (IJCAI)},
	eventdate    = {2011-07-16/2011-07-22},
	eventtitle   = {22th International Joint Conference on Artificial Intelligence (IJCAI)},
	keywords     = {ijcai},
	read         = {Yes},
	venue        = {Barcelona, Spain}
}


@article{girinDynamicalVariationalAutoencoders2021,
  title = {Dynamical {{Variational Autoencoders}}: {{A Comprehensive Review}}},
  shorttitle = {Dynamical {{Variational Autoencoders}}},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and {Alameda-Pineda}, Xavier},
  year = {2021},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {15},
  number = {1-2},
  eprint = {2008.12595},
  eprinttype = {arxiv},
  pages = {1--175},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000089},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
}

@article{mittelmanStructuredRecurrentTemporal,
  title = {Structured {{Recurrent Temporal Restricted Boltzmann Machines}}},
  author = {Mittelman, Roni and Kuipers, Benjamin and Savarese, Silvio and Lee, Honglak},
  pages = {9},
  langid = {english},
  keywords = {_tablet},
}




@article{DBLP:journals/corr/abs-1708-06004,
  author    = {Takayuki Osogami},
  title     = {Boltzmann machines for time-series},
  journal   = {CoRR},
  volume    = {abs/1708.06004},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.06004},
  eprinttype = {arXiv},
  eprint    = {1708.06004},
  timestamp = {Mon, 13 Aug 2018 16:46:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-06004.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{osogamiBoltzmannMachinesEnergybased2019,
  title = {Boltzmann Machines and Energy-Based Models},
  author = {Osogami, Takayuki},
  year = {2019},
  month = Jan,
  journal = {arXiv:1708.06008 [cs]},
  eprint = {1708.06008},
  eprinttype = {arxiv},
  primaryclass = {cs},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{osogamiBoltzmannMachinesTimeseries2019,
  title = {Boltzmann Machines for Time-Series},
  author = {Osogami, Takayuki},
  journal   = {CoRR},
  volume    = {abs/1708.06004},
  year      = {2019},
  url       = {http://arxiv.org/abs/1708.06004},
  eprinttype = {arXiv},
  eprint    = {1708.06004},
}


@article{st_rkm,
  author    = {Arun Pandey and
               Micha{\"{e}}l Fanuel and
               Joachim Schreurs and
               Johan A. K. Suykens},
  title     = {Disentangled Representation Learning and Generation with Manifold
               Optimization},
  doi = {10.48550/ARXIV.2006.07046},
  journal   = {{{To appear in Neural Computation. CoRR}}},
  volume    = {abs/2006.07046},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.07046},
  eprinttype = {arXiv},
  eprint    = {2006.07046},
  timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-07046.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sutskeverRecurrentTemporalRestricted,
 author = {Sutskever, Ilya and Hinton, Geoffrey E. and Taylor, Graham W.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {},
 pages = {},
 publisher = {},
 title = {The Recurrent Temporal Restricted Boltzmann Machine},
 url = {https://proceedings.neurips.cc/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf},
 volume = {21},
 year = {2008}
}




@inproceedings{kpca_denoising,
 author = {Mika, Sebastian and Sch\"{o}lkopf, Bernhard and Smola, Alex and M\"{u}ller, Klaus-Robert and Scholz, Matthias and R\"{a}tsch, Gunnar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 pages = {},
 publisher = {MIT Press},
 title = {Kernel PCA and De-Noising in Feature Spaces},
 url = {https://proceedings.neurips.cc/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf},
 volume = {11},
 year = {1998}
}

@article{TONIN2021661,
title = {Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints},
journal = {Neural Networks},
volume = {142},
pages = {661-679},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021002860},
author = {Francesco Tonin and Panagiotis Patrinos and Johan A.K. Suykens},
keywords = {Kernel methods, Unsupervised learning, Manifold learning, Learning disentangled representations},
}

@article{KALLAS20133053,
title = {Kernel autoregressive models using {{Yule-Walker}} equations},
journal = {Signal Processing},
volume = {93},
number = {11},
pages = {3053-3061},
year = {2013},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2013.03.032},
url = {https://www.sciencedirect.com/science/article/pii/S0165168413001242},

author = {Maya Kallas and Paul Honeine and Clovis Francis and Hassan Amoud},
keywords = {Kernel machines, Autoregressive model, Time series prediction, Yule–Walker equations, Pre-image problem},
abstract = {This paper proposes nonlinear autoregressive (AR) models for time series, within the framework of kernel machines. Two models are investigated. In the first proposed model, the AR model is defined on the mapped samples in the feature space. In order to predict a future sample, this formulation requires to solve a pre-image problem to get back to the input space. We derive an iterative technique to provide a fine-tuned solution to this problem. The second model bypasses the pre-image problem, by defining the AR model with an hybrid model, as a tradeoff considering the computational time and the precision, by comparing it to the iterative, fine-tuned, model. By considering the stationarity assumption, we derive the corresponding Yule–Walker equations for each model, and show the ease of solving these problems. The relevance of the proposed models is studied on several time series, and compared with other well-known models in terms of accuracy and computational complexity.}
}

